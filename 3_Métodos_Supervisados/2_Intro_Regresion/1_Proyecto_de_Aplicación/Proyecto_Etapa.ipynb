{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZnhDUgaOWt4"
   },
   "source": [
    "#**Fundamentos de Machine Learning - Proyecto de Aplicación 1: Regresión**\n",
    "\n",
    "**Objetivos**\n",
    "* Aplicar técnicas de regresión para construir un modelo predictivo que permita estimar la demanda sobre el uso de un sistema de alquiler de bicicletas siguiendo el ciclo de machine learning.\n",
    "* Determinar cuáles son los factores que más inciden en la demanda con base en los datos.\n",
    "\n",
    "**Conjunto de datos**\n",
    "\n",
    "El conjunto de datos recoge información sobre la cantidad de bicicletas rentadas en un período de tiempo, junto con información meteorológica y de temporalidad, entre otros.\n",
    "\n",
    "\n",
    "**Actividades**\n",
    "1. Eploración y perfilamiento de los datos, empleando funcionalidades de la librería pandas.\n",
    "\n",
    "2. Limpieza y preparación de los datos, justificando las decisiones tomadas con base en los resultados de la fase de exploración.\n",
    "\n",
    "3. Construcción de un modelo de regresión polinomial.  Para determinar el grado de la transformación polinomial se recomienda emplear técnicas de selección de modelos sobre los valores de polinomio [2,3]. Emplee la métrica RMSE.\n",
    "\n",
    "4. Construcción de un modelo de regresión regularizada Lasso. Para determinar el valor del hiperparámetro de regularización, emplee técnicas de selección de modelos sobre los siguientes valores [1,2,3,4,5] y emplee la métrica RMSE.\n",
    "\n",
    "5. Elaboración de una tabla comparativa mostrando el rendimiento sobre test de los dos modelos seleccionados (con mejores rendimientos) de las actividades 3 y 4, con las métricas R2, RMSE y MAE.\n",
    "\n",
    "6. Con base en el modelo Lasso determinar las variables más importantes para la predicción.\n",
    "\n",
    "**Consideraciones:**\n",
    "* Emplee el valor de semilla de 77 (random_state).\n",
    "* En las técnicas de selección de modelos uno de los parámetros que debes indicar es `scoring = 'neg_root_mean_squared_error'`, con el fin de seleccionar RMSE como métrica de evaluación en la búsqueda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pi9zkFfoQeGs"
   },
   "source": [
    "# **1.Importación de datos y dependencias**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxDw5HwnfQL-"
   },
   "source": [
    "### 1.1. Dependencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BC8mp6kne4a9"
   },
   "source": [
    "##### 1.1.1. Librerías principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "u8QCW4zLSI9P"
   },
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modelamiento\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, RobustScaler, OrdinalEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import BaggingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIG2r6Rje0-G"
   },
   "source": [
    "##### 1.1.2. Utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "BnMXurAIe-am"
   },
   "outputs": [],
   "source": [
    "def excel_to_csv(path, sheet_name):\n",
    "  df_excel = pd.read_excel(path, sheet_name=sheet_name)\n",
    "  columnas = df_excel.columns.tolist()[0].split(',')\n",
    "  data_dict = {columna: [] for columna in columnas}\n",
    "  for registro in df_excel.iloc[1:].values:\n",
    "    registro = registro[0]\n",
    "    valores = registro.split(',')\n",
    "    for i, columna in enumerate(columnas):\n",
    "      data_dict[columna].append(valores[i])\n",
    "  data_raw = pd.DataFrame(data_dict)\n",
    "  return data_raw\n",
    "\n",
    "\n",
    "def trasnformar_tipo_datos(data, tipos_de_datos):\n",
    "  df = data.copy()\n",
    "  for columna, tipo in tipos_de_datos.items():\n",
    "    df[columna] = df[columna].astype(tipo)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "t-QMW9CjrNBS"
   },
   "outputs": [],
   "source": [
    "def histogram_box(dataframe, variable):\n",
    "  df = dataframe\n",
    "  fig, ax = plt.subplots(1,2,figsize=(14,5))\n",
    "  sns.histplot(data=df, x=str(variable), kde=True, ax=ax[0])\n",
    "  sns.boxplot(data=df, x=str(variable))\n",
    "\n",
    "  mean = df[str(variable)].mean()\n",
    "  ax[0].axvline(mean, color='red')\n",
    "\n",
    "  variance = round(df[str(variable)].var(),2)\n",
    "  kurtosis = round(df[str(variable)].kurt(),2)\n",
    "  skewness = round(df[str(variable)].skew(),2)\n",
    "\n",
    "  textstr = '\\n'.join( (r'mean=%.2f' % (mean, ), r'Variance=%.2f' % (variance, ), r'Kurtosis=%.2f' % (kurtosis, ), r'Skewness=%.2f' % (skewness, )) )\n",
    "  props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "  ax[0].text(0.05, 0.95, textstr, transform=ax[0].transAxes, fontsize=10,\n",
    "             verticalalignment='top', bbox=props)\n",
    "  ax[0].set_title(f'Histogram of {variable}')\n",
    "  ax[0].set_xlabel(f'{variable}')\n",
    "  ax[0].set_ylabel(\"Frecuency\")\n",
    "  ax[0].grid(True)\n",
    "  ax[1].set_title(f'Boxplot of {variable}')\n",
    "  ax[1].set_xlabel(f'{variable}')\n",
    "  ax[1].set_ylabel(\"Frecuencias\")\n",
    "  ax[1].grid(True)\n",
    "\n",
    "  plt.subplots_adjust(hspace=0.5)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "w1QbOmJUpFvF"
   },
   "outputs": [],
   "source": [
    "class data_processing:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def null_classifier(self,dataframe):\n",
    "    series = dataframe.isnull().sum() * 100 / dataframe.shape[0]\n",
    "    nulls  = {'Variable': series.index, 'Percentage of null values': series.values}\n",
    "    nulls = pd.DataFrame(nulls).sort_values(by = 'Percentage of null values', ascending = False)\n",
    "    return nulls\n",
    "  \n",
    "  def column_selector(self, dataframe, column_names):\n",
    "    df = dataframe\n",
    "    selected_columns = [col for col in column_names if col in df.columns]\n",
    "    selected_df = df[selected_columns]\n",
    "    return selected_df\n",
    "\n",
    "  def null_df_filter(self, dataframe, threshold):\n",
    "    df = dataframe\n",
    "    nulls = self.null_classifier(df)\n",
    "    filtered = nulls[nulls['Percentage of null values'] < threshold]['Variable']\n",
    "    df_filtered = df[filtered]\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlUL-DF0ex6D"
   },
   "source": [
    "### 1.3. Importación y descripción de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0b7e3ihOSDpC"
   },
   "source": [
    "---\n",
    "Importación y descripción de los datos\n",
    "\n",
    "---\n",
    "Los sistemas de bicicletas compartidas son una innovación en alquileres de bicicletas, donde todo el proceso es automático. Actualmente, hay más de 500 programas de bicicletas compartidas en el mundo, con más de 500 mil bicicletas. Estos sistemas son importantes para el tráfico, el medio ambiente y la salud. Además, los datos generados por estos sistemas son útiles para la investigación, ya que registran la duración del viaje y las posiciones de inicio y fin, convirtiéndolos en una red de sensores virtual para detectar la movilidad en la ciudad y eventos importantes.\n",
    "\n",
    "El diccionario de los datos es el siguiente:\n",
    "\n",
    "|Variable|Descripción|Tipología|\n",
    "|---|---|---|\n",
    "|season| Estación del año (Winter, Spring, Summer, Fall)|categórica|\n",
    "|weekday|Día de la semana (de 1 a 7)|numérico|\n",
    "|weathersit|Clima (Clear, Mist, Light Rain, Heavy Rain)|categórica|\n",
    "|temp|Temperatura|numérico|\n",
    "|atemp|Sensación de temperatura|numérico|\n",
    "|hum|Humedad|numérico|\n",
    "|windspeed|Velocidad del viento|numérico|\n",
    "|cnt|Cantidad de bicicletas rentadas|numérico|\n",
    "|time_of_day|Parte del día (Morning, Evening, Night)|categórica|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afeli\\Downloads\\Regression\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afeli\\Downloads\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0r2sZeaX5sR",
    "outputId": "8c1cac44-4e8f-4cb8-97c3-4a0c58542463"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"day.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S9mGKEsLf11w",
    "outputId": "bb9e91f6-8742-4256-da88-fa4c21032322"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, '2011-01-02', 1, 0, 1, 0, 0, 0, 2, 0.363478, 0.353739, 0.696087, 0.248539, 131, 670, 801]\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[1].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0lZauWMfeCi"
   },
   "source": [
    "Como podemos observar, exíste una primera compliación con los datos. El formato en el que vienen almacenados corresponde a un archivo tipo Excel que al importarlo por métodos tradicionales en Pandas, genera un dataframe de 17379 regístros y una columna, lo cual significa que todos los datos de todas las columnas están siendo apiladas en una sola columna.\n",
    "\n",
    "Por otra parte, como se muéstra en la celda anterior, cada regístro crudo está presentado en el dataset como una lísta de un único elemento de tipo string. Así, para poder procesar nuestros datos tenemos que:\n",
    "1. Tomar el primer regístro como los headers de las columnas.\n",
    "2. Tomar cada regístro del archivo excel, extrare el único elemento y dividir el string mediante el método `.split(',')` de modo que se genere una lísta con todos los valores correspondientes a cada columna para el regístro en cuestion.\n",
    "3. Crear un diccionario vacío cuyas llaves sean los nombres de las columnas.\n",
    "4. Generar un iterador que rellene los valores de los diccionarios para cada columna y recorra todos los regístros del dataset.\n",
    "5. Reconstruir el dataset como un DataFrame.\n",
    "\n",
    "Éstos procedimientos son llevados a cabo por la función `excel_to_csv` presente en la sección de utilidades 1.1.2. del presente notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5r2iqn-UuEE"
   },
   "source": [
    "Una vez importados los datos, procedamos a realizar exploraciones básicas de los mísmos tales como la verificación del tipo de datos que poseemos, el tamaño del dataset y las estadísticas descriptivas básicas de cada variable para tener una perspectiva inicial de la información que disponemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y4Z0u_JQVRUY",
    "outputId": "4d4426c1-592e-4bdd-821f-5b1bef90363b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(731, 16)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPCowQvJhLjg"
   },
   "source": [
    "Descripción de las variables del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qQn4_0wUhOAe",
    "outputId": "1cdf4381-63ff-4c49-9f21-862055cd3b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 731 entries, 0 to 730\n",
      "Data columns (total 16 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   instant     731 non-null    int64  \n",
      " 1   dteday      731 non-null    object \n",
      " 2   season      731 non-null    int64  \n",
      " 3   yr          731 non-null    int64  \n",
      " 4   mnth        731 non-null    int64  \n",
      " 5   holiday     731 non-null    int64  \n",
      " 6   weekday     731 non-null    int64  \n",
      " 7   workingday  731 non-null    int64  \n",
      " 8   weathersit  731 non-null    int64  \n",
      " 9   temp        731 non-null    float64\n",
      " 10  atemp       731 non-null    float64\n",
      " 11  hum         731 non-null    float64\n",
      " 12  windspeed   731 non-null    float64\n",
      " 13  casual      731 non-null    int64  \n",
      " 14  registered  731 non-null    int64  \n",
      " 15  cnt         731 non-null    int64  \n",
      "dtypes: float64(4), int64(11), object(1)\n",
      "memory usage: 91.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZQfTw0vhbD-"
   },
   "source": [
    "Como vemos, todos los datos están en formato string, así pues, tenemos que transformar el tipo de datos para cada columna. Para ello, empleamos la función transformar_tipo_datos presente en la sección de utilidades 1.1.2. del presente notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydGAabFrhmXA",
    "outputId": "cfc748b9-9947-4d24-ad6b-66f13d3c9cde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntipos = {\\n    'season':str,\\n    'weekday': int,\\n    'weathersit': str,\\n    'temp': float,\\n    'atemp':float,\\n    'hum':float,\\n    'windspeed':float,\\n    'cnt': int,\\n    'time_of_day':str\\n}\\n\\ndf = trasnformar_tipo_datos(df, tipos)\\ndf.info()\\n\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "tipos = {\n",
    "    'season':str,\n",
    "    'weekday': int,\n",
    "    'weathersit': str,\n",
    "    'temp': float,\n",
    "    'atemp':float,\n",
    "    'hum':float,\n",
    "    'windspeed':float,\n",
    "    'cnt': int,\n",
    "    'time_of_day':str\n",
    "}\n",
    "\n",
    "df = trasnformar_tipo_datos(df, tipos)\n",
    "df.info()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubUCQk_6QiVg"
   },
   "source": [
    "# **2.Análisis Exploratorio de Datos  (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gdkTXLnm8J8"
   },
   "source": [
    "### 2.1. Derección de valores nulos y duplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ksca6m3nA4v"
   },
   "source": [
    "---\n",
    "Detección de valores nulos y duplicados\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGNljjUYkVFn"
   },
   "source": [
    "Una vez los datos han quedado en el formato estructurado adecuado, procedemos a explorar los datos mediante visualizaciones y estadísticos principales. Para empezar, observemos la estadística descriptiva básica de nuestros datos numéricos y categóricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "2VVdSqkgkgpJ",
    "outputId": "618ef13c-d845-434a-ef97-247166963bc6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "      <td>731.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>366.000000</td>\n",
       "      <td>2.496580</td>\n",
       "      <td>0.500684</td>\n",
       "      <td>6.519836</td>\n",
       "      <td>0.028728</td>\n",
       "      <td>2.997264</td>\n",
       "      <td>0.683995</td>\n",
       "      <td>1.395349</td>\n",
       "      <td>0.495385</td>\n",
       "      <td>0.474354</td>\n",
       "      <td>0.627894</td>\n",
       "      <td>0.190486</td>\n",
       "      <td>848.176471</td>\n",
       "      <td>3656.172367</td>\n",
       "      <td>4504.348837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>211.165812</td>\n",
       "      <td>1.110807</td>\n",
       "      <td>0.500342</td>\n",
       "      <td>3.451913</td>\n",
       "      <td>0.167155</td>\n",
       "      <td>2.004787</td>\n",
       "      <td>0.465233</td>\n",
       "      <td>0.544894</td>\n",
       "      <td>0.183051</td>\n",
       "      <td>0.162961</td>\n",
       "      <td>0.142429</td>\n",
       "      <td>0.077498</td>\n",
       "      <td>686.622488</td>\n",
       "      <td>1560.256377</td>\n",
       "      <td>1937.211452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.059130</td>\n",
       "      <td>0.079070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022392</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>183.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.337083</td>\n",
       "      <td>0.337842</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.134950</td>\n",
       "      <td>315.500000</td>\n",
       "      <td>2497.000000</td>\n",
       "      <td>3152.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>366.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.498333</td>\n",
       "      <td>0.486733</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.180975</td>\n",
       "      <td>713.000000</td>\n",
       "      <td>3662.000000</td>\n",
       "      <td>4548.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>548.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.655417</td>\n",
       "      <td>0.608602</td>\n",
       "      <td>0.730209</td>\n",
       "      <td>0.233214</td>\n",
       "      <td>1096.000000</td>\n",
       "      <td>4776.500000</td>\n",
       "      <td>5956.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>731.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.861667</td>\n",
       "      <td>0.840896</td>\n",
       "      <td>0.972500</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>3410.000000</td>\n",
       "      <td>6946.000000</td>\n",
       "      <td>8714.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          instant      season          yr        mnth     holiday     weekday  \\\n",
       "count  731.000000  731.000000  731.000000  731.000000  731.000000  731.000000   \n",
       "mean   366.000000    2.496580    0.500684    6.519836    0.028728    2.997264   \n",
       "std    211.165812    1.110807    0.500342    3.451913    0.167155    2.004787   \n",
       "min      1.000000    1.000000    0.000000    1.000000    0.000000    0.000000   \n",
       "25%    183.500000    2.000000    0.000000    4.000000    0.000000    1.000000   \n",
       "50%    366.000000    3.000000    1.000000    7.000000    0.000000    3.000000   \n",
       "75%    548.500000    3.000000    1.000000   10.000000    0.000000    5.000000   \n",
       "max    731.000000    4.000000    1.000000   12.000000    1.000000    6.000000   \n",
       "\n",
       "       workingday  weathersit        temp       atemp         hum   windspeed  \\\n",
       "count  731.000000  731.000000  731.000000  731.000000  731.000000  731.000000   \n",
       "mean     0.683995    1.395349    0.495385    0.474354    0.627894    0.190486   \n",
       "std      0.465233    0.544894    0.183051    0.162961    0.142429    0.077498   \n",
       "min      0.000000    1.000000    0.059130    0.079070    0.000000    0.022392   \n",
       "25%      0.000000    1.000000    0.337083    0.337842    0.520000    0.134950   \n",
       "50%      1.000000    1.000000    0.498333    0.486733    0.626667    0.180975   \n",
       "75%      1.000000    2.000000    0.655417    0.608602    0.730209    0.233214   \n",
       "max      1.000000    3.000000    0.861667    0.840896    0.972500    0.507463   \n",
       "\n",
       "            casual   registered          cnt  \n",
       "count   731.000000   731.000000   731.000000  \n",
       "mean    848.176471  3656.172367  4504.348837  \n",
       "std     686.622488  1560.256377  1937.211452  \n",
       "min       2.000000    20.000000    22.000000  \n",
       "25%     315.500000  2497.000000  3152.000000  \n",
       "50%     713.000000  3662.000000  4548.000000  \n",
       "75%    1096.000000  4776.500000  5956.000000  \n",
       "max    3410.000000  6946.000000  8714.000000  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibTUqcOKlOCK"
   },
   "source": [
    "De los estadísticos principales podemos observar:\n",
    "* En promedio, se alquila una cantidad de 189 bicicletas en el periodo de tiempo sobre el cuál los datos fueron recogidos.\n",
    "* En promedio, los alquileres se realizan el cuarto día de la semana (jueves).\n",
    "* El 25% de los alquileres suceden el segundo dia de la semana (martes), el 50% en el cuarto (jueves), el 75% en el sexto (sabado).\n",
    "* El 25% del total de alquileres corresponde a 40, el 50% de los alquileres corresponde a 142 mientras que el 75% del total de los alquileres corresponde a 281.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAqiJ5fWnHGf"
   },
   "source": [
    "Notamos por otra parte que los valores de la variable `weekday` van desde 0 a 6, información que no es consistente con el diccionario de los datos en el cual se estipula que los días van desde 1 a 7. Para mantener la congruencia entre éstos dos objetos, datos y diccionario, realizamos una transformación numérica sobre los datos de la siguiente manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C9bRg0R1neie",
    "outputId": "5154d4de-2be1-4521-8fd1-2592433ee577"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 1, 2, 3, 4, 5, 6], dtype=int64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['weekday'] = df['weekday'] + 1\n",
    "df['weekday'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g12NU6y5nsQi"
   },
   "source": [
    "Note entonces, ya que en el diccionario de los datos no se especifica la convención correcta entre el indice y el día de la semana, procedemos a usar la siguiente convención: `domingo:7` , `lunes:1`, `martes:2`, `miercoles:3`, `jueves:4`, `viernes:5`, `sábado:6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Sxp68gbRkvlI",
    "outputId": "1fe400b9-194b-451c-b22b-6adbfcde86fc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dteday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2011-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            dteday\n",
       "count          731\n",
       "unique         731\n",
       "top     2011-01-01\n",
       "freq             1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27Qg5RuwoIrJ"
   },
   "source": [
    "Según la frecuencia de las variables categóricas, se observa que:\n",
    "* El verano (summer) es la temporada anual en donde más se rentan bicicletas.\n",
    "* El tipo de clima preferido por las personas para tomar paseos en bicicleta es 'Clear'.\n",
    "* El horario del día donde las personas toman más paseos en bicicleta es la noche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdpkAukKpmeW"
   },
   "source": [
    "No obstante, ¿estas observaciones iniciales son acertadas?. Para contestar esta pregunta, tenemos que determinar si dentro del dataset existen valores nulos por columna para descartar la posibilidad de que nuestras observaciones estén sesgadas debido a la presencia de valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0tVnU_5kpWZ8",
    "outputId": "aa48e119-319e-41f3-aebf-29d38fb568df"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vr2FNiAz-v0F"
   },
   "source": [
    "Ahora, procedemos a responder la pregunta ¿exíste un número importante de duplicados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PBxLkVGi1DPf",
    "outputId": "cdfc152e-427d-4d04-fcea-92dee804bb40"
   },
   "outputs": [],
   "source": [
    "n_duplicados = df.duplicated().sum()\n",
    "porcentaje_duplicados = (n_duplicados / df.shape[0]) *100\n",
    "print('El número de duplicados es = ', n_duplicados)\n",
    "print('Porcentaje de duplicados es =', porcentaje_duplicados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwHZ90BH6Io_"
   },
   "source": [
    "Ya que el número de duplicados solo corresponde al 0.241% de los datos, las distribuciones no se deben ver fuertemente afectadas por su presencia, por ende, continuaremos el análisis exploratorio de los datos eliminando dichos suplicados sin temor a causar estragos en las distribuciones de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gHfB5vOc-9cu",
    "outputId": "176596b7-af74-4cdb-89bc-ee6cdc3ebe91"
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "n_duplicados = df.duplicated().sum()\n",
    "porcentaje_duplicados = (n_duplicados / df.shape[0]) *100\n",
    "print('El número de duplicados es = ', n_duplicados)\n",
    "print('Porcentaje de duplicados es =', porcentaje_duplicados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GCem4PG-sIsy",
    "outputId": "1aff521c-bacd-4d66-bd7e-d81d5edad36c"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtKRSO0-nOOm"
   },
   "source": [
    "### 2.2. Análisis univariado de variables numéricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6icT3Q1xp7o_"
   },
   "source": [
    "Ya que no exísten valores nulos en nuestras variables, procedemos con el análisis univariado de nuestro dataset. Para empezar, observemos las distribuciones de las variables numéricas contínuas y para la variable `'cnt'` e intentemos detectar posibles valores atípicos o outliers. Para ésto, emplearemos una visualización de histograma y boxplot mediante la función `histogram_box`, presente en la sección de utilidades 1.1.2. del presente notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "103wWl4er8WY",
    "outputId": "814e3cdc-da80-4809-a4ca-8c6956feebec"
   },
   "outputs": [],
   "source": [
    "histogram_box(df, 'cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-41uCFDsvdA"
   },
   "source": [
    "A partir de la visualizacion anterior, podemos observar lo siguiente:\n",
    "* El número de alquileres de bicicletas presenta una distribución de sus datos casi monotónicamente decreciente con respecto al número de alquileres, lo cual quiere decir que exíste una gran cantidad de personas que realiza pocos alquileres de biscicletas y una pequeña cantidad de personas que realiza una alta cantidad de alquileres. Por ende, la distribución de los datos posee una alta varianza y un sesgo (skewness) positivo. Lo que implica una distribución asimétrica.\n",
    "\n",
    "* La gran mayoria de las personas realiza un número de alquileres entre 40 y 281. No obstante, exísten unos valores atípicos que corresponden a un selecto número de personas que realiza más de 650 alquileres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "rgxCN7qxwfeo",
    "outputId": "57e017cc-810d-4e87-ee5e-f8551d76efae"
   },
   "outputs": [],
   "source": [
    "histogram_box(df, 'temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bL3V1IA5uAZa"
   },
   "source": [
    "A partir de la visualizacion anterior, podemos observar lo siguiente:\n",
    "* La distribución del alguiler de bicicletas con respecto a la temperatura tiende a ser bimodal. Esto quiere decir que las personas prefieren realizr alquileres de bicicletas especialmente en dos valores de temperatura.\n",
    "\n",
    "* Las personas prefieren alquilar bicicletas en promedio, cuando hace una temperatura de 15.36 grados.\n",
    "\n",
    "* La distribución del alquiler de bicicletas con respecto a la temperatura es prácticamente simétrica lo que se traduce en un skewness cercano a cero. No se puede decir que la distribución tenga una fuerte presencia de \"colas\" y la mayoría de los datos se encuentra concentrados en un amplio rango ya que su kurtosis es menor a 3, por ende, es una distribuciínplaticúrtica.\n",
    "\n",
    "* Con respecto a la temperatura, no exísten valores atípicos, por ende, los datos se encuentran altamente concentrados entre 7.9 y 23 grados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "4KRjhcuqt18k",
    "outputId": "5e06cadf-cfdf-461d-df5f-3167f0bc104d"
   },
   "outputs": [],
   "source": [
    "histogram_box(df, 'atemp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XV9IKkCgxCzx"
   },
   "source": [
    "A partir de la visualizacion anterior, podemos observar lo siguiente:\n",
    "* La distribución del número de alquileres con respecto a la sensacion de la temperatura es demasiado similar a la de la temperatura, a diferencia de que en éste caso la variable tiende a tener más modas.\n",
    "\n",
    "* En promedio, las presonas prefieren alquilar bicicletas cuando exíste una sensación térmica cercana a 15.40 grados.\n",
    "\n",
    "* La distribución es prácticamente simétrica, sin presencia fuerte de colas y sin presencia de valores atípicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "t3_u5_Dexjiz",
    "outputId": "929c0d16-cebf-4bed-994f-0063d6af2d63"
   },
   "outputs": [],
   "source": [
    "histogram_box(df, 'hum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgtblZ7SxuQW"
   },
   "source": [
    "A partir de la visualizacion anterior, podemos observar lo siguiente:\n",
    "* El alquiler de bicicletas ocurre en promedio para humedades moderadas de 0.6.\n",
    "* La distribución de los alquileres en función de la humedad posee sesgo hacia la izquierda presentando sesgo negativo. Específicamente, la mayoría de los alquileres ocurre en un ampluio rango al rededor del valor de humedad 0.6  que va aproximadamente desde 0.4 hasta 0.9.\n",
    "* Exíste un alto número de alguileres entre el valor de humedad 0.8 y 0.9.\n",
    "* Corresponde a una distribución platicúrtica.\n",
    "* Exíste un pequeño número de valores atípicos los cuales corresponden a alquileres hechos a muy bajas humedades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "wDm1N24h0RgR",
    "outputId": "a48fcf7e-57e4-42ec-e5f4-08dda490cfe4"
   },
   "outputs": [],
   "source": [
    "histogram_box(df, 'windspeed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSmkAtBH0ZA2"
   },
   "source": [
    "A partir de la visualizacion anterior, podemos observar lo siguiente:\n",
    "* La distribución del número de alquileres en función de la velocidad del viento es ligeramente asimétrica hacia la derecha, presentando sesgo positivo.\n",
    "\n",
    "* En promedio, los alquileres de bicicletas ocurren cuando exíste una velocidad del viento de 12.74.\n",
    "\n",
    "* Existe una presencia de valores atípicos los cuales corresponden a alquileres hechos a altas velocidades del viento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hEk6S2IoQJc"
   },
   "source": [
    "### 2.3. Análisis de variables categóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrzc7jcv_UBO"
   },
   "source": [
    "Una vez analizadas las variables numéricas no ordinales y contínuas, procedamos a analizar que ocurre con las variables categóricas y ordinales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "vZZujzH-_dHs",
    "outputId": "aa21a3b5-a74e-46ec-94cf-b611e45fbe8b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "sns.countplot(data=df,x='season', palette = 'muted')\n",
    "plt.title('Número de clientes con respecto a la temporada')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "_XqZefIL_tPp",
    "outputId": "c9d9ab60-cc22-4fdd-e077-a4d1ffc7f50d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "sns.countplot(data=df,x='time_of_day',palette = 'muted')\n",
    "plt.grid(True)\n",
    "plt.title('Número de clientes con respecto al tiempo del dia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "sYbct1hAAAu0",
    "outputId": "1f581d10-99f7-44a7-d0d2-084198dadb84"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "sns.countplot(data=df,x='weekday',palette = 'muted')\n",
    "plt.title('Número de clientes con respecto al día')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "01u2v1t6AQlL",
    "outputId": "9ce03909-0b05-4a11-b95f-9f3b94877180"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "sns.countplot(data=df,x='weathersit',palette = 'muted')\n",
    "plt.title('Número de clientes con respecto al tipo de clima')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znHvdTiBBr1G"
   },
   "source": [
    "Con respecto a los plots de conteo anteriores, con respecto al número de regístros de alquiler podémos concluir lo siguiente:\n",
    "* La mayoría de regístros ocurren en verano.\n",
    "\n",
    "* El tipo de clima es muy relevante, la mayoría de alquileres ocurre en clima despejado.\n",
    "\n",
    "* No exíste una pronunciada jerarquía entre los días de la semana, no obstante, con respecto al conteo de alquileres, la mayoría de éstos ocurre en el día 7 y 1.\n",
    "\n",
    "* La mayoría de regístros reportados según el periodo del día ocurren en la noche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCOKyU-p6-Qd"
   },
   "source": [
    "A pesar de que el número de regístros ocurridos de acuerdo a una categoría nos da indicios a cerca del comportamiento de la renta de bicicletas con respecto a cierta variable, no nos da ninguna información a cerca de **cómo se comporta el número de alquileres efectuado por una persona en función de la categoría**.\n",
    "\n",
    "Para obtener información en éste aspecto, procedemos a realizar comparaciones entre el número de alquileres efectuados por una persona (no el número de regístros) y variables como el día de la semana, el tiempo del día, el tipo de clima y la temporada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "HLxgm4-m21TX",
    "outputId": "f1f58928-642e-4ec6-b6d4-458f03c9c877"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2,figsize=(10,5))\n",
    "sns.barplot(data=df, x=\"weekday\", y=\"cnt\", ax=ax[0,0],palette = 'muted')\n",
    "sns.boxplot(data=df, x=\"weekday\", y=\"cnt\", ax=ax[0,1],palette = 'muted')\n",
    "sns.barplot(data=df, x='time_of_day', y=\"cnt\", ax=ax[1,0],palette = 'muted')\n",
    "sns.boxplot(data=df, x='time_of_day', y=\"cnt\", ax=ax[1,1],palette = 'muted')\n",
    "\n",
    "ax[0,0].set_title(\"Barplot del número de alquileres\")\n",
    "ax[0,0].set_xlabel(\"Día de la semana\")\n",
    "ax[0,0].set_ylabel(\"Alquileres (promedio)\")\n",
    "ax[0,1].set_title(\"Boxplot del número de alquileres\")\n",
    "ax[0,1].set_xlabel(\"Día de la semana\")\n",
    "ax[0,1].set_ylabel(\"Alquileres (número)\")\n",
    "ax[1,0].set_title(\"Barplot del número de alquileres\")\n",
    "ax[1,0].set_xlabel(\"Periodo del dia\")\n",
    "ax[1,0].set_ylabel(\"Alquileres (promedio)\")\n",
    "ax[1,1].set_title(\"Boxplot del número de alquileres\")\n",
    "ax[1,1].set_xlabel(\"Periodo del dia\")\n",
    "ax[1,1].set_ylabel(\"Alquileres (número)\")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nk7uoXBk745W"
   },
   "source": [
    "A partir de la comparación entre el número de alquileres y variables tales como el día de la semana y periodo del día podemos observar que:\n",
    "\n",
    "* No exíste una pronunciada jerarquía entre los días de la semana con respecto al número de alquileres por persona. No obstante, es el día 5 y 6 aquellos en donde más alquileres ocurren.\n",
    "\n",
    "* Durante todos los días de la semana, exíste un alto número de personas que han realizado un alto número de alquileres. Es decir, el día de la semana no parece influir en la presencia de outliers (personas que rentan bicicletas con alta frecuencia).\n",
    "\n",
    "* El periodo del día donde más personas con alta frecuencia de alquiler de bicicletas ocurren es en la tarde, luego en la mañana y por último, la noche. Es decir, a pesar de que según el periodo del día, la mayoría de regístros ocurren en la noche, las personas que alquilan bicicletas en la noche suelen alquilar bicicletas muy poco frecuente, mientras que las personas que alquílan bicicletas en la tarde, suelen ser personas que frecuentemente alquilan bicicletas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "xLYNq3eg7UdT",
    "outputId": "4e23e331-3533-4ec5-94b2-2b9ebe280e86"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2,figsize=(10,5))\n",
    "sns.barplot(data=df, x='weathersit', y=\"cnt\", ax=ax[0,0],palette = 'muted')\n",
    "sns.boxplot(data=df, x='weathersit', y=\"cnt\", ax=ax[0,1],palette = 'muted')\n",
    "sns.barplot(data=df, x='season', y=\"cnt\", ax=ax[1,0],palette = 'muted')\n",
    "sns.boxplot(data=df, x=\"season\", y=\"cnt\", ax=ax[1,1],palette = 'muted')\n",
    "\n",
    "ax[0,0].set_title(\"Barplot del número de alquileres\")\n",
    "ax[0,0].set_xlabel(\"Tipo de clima\")\n",
    "ax[0,0].set_ylabel(\"Alquileres (promedio)\")\n",
    "ax[0,1].set_title(\"Boxplot del número de alquileres\")\n",
    "ax[0,1].set_xlabel(\"Tipo de clima\")\n",
    "ax[0,1].set_ylabel(\"Alquileres (número)\")\n",
    "ax[1,0].set_title(\"Barplot del número de alquileres\")\n",
    "ax[1,0].set_xlabel(\"temporada\")\n",
    "ax[1,0].set_ylabel(\"Alquileres (promedio)\")\n",
    "ax[1,1].set_title(\"Boxplot del número de alquileres\")\n",
    "ax[1,1].set_xlabel(\"temporada\")\n",
    "ax[1,1].set_ylabel(\"Alquileres (número)\")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdetJ4rz1jMH"
   },
   "source": [
    "A partir de la comparación entre el número de alquileres y variables tales como el tipo de clima y la temporada podemos observar que:\n",
    "\n",
    "* Exíste una correspondencia entre el número de alquileres y la temporada, en donde más ocurren alquileres es en verano, primavera, otoño y por último invierno.\n",
    "\n",
    "* La temporada que más presenta valores atípicos, es decir, un alto número de alquileres efectuados por un pequeño grupo de personas, es en invierno.\n",
    "\n",
    "* Exíste una fuerte correspondencia entre el tipo de clima y el número de alquileres, en donde el clima favorito por las personas para tomar paseos en bicicleta es despejado, luego con neblina, lluvioso suave y por último, lluvioso.\n",
    "\n",
    "* En general, el número de regístros ocurridos en determinada temporada se corresponde con la jerarquía existente entre el número de alquileres por persona en determinada temporada. Así, la mayoría de de clientes alquila en verano y a quellos que alquilan bicicletas suelen tener una alta frecuencia de alquiler.\n",
    "\n",
    "* En general, el número de regístros en determinado tipo de clima se corresponde con la jerarquía existente entre el número de alquileres por persona en determinado tiempo del día. De ésta manera, la mayoría de clientes que alquila en clima despejado son aquelos que alquilan bicicletas frecuentemente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAqqYLW8o83e"
   },
   "source": [
    "### 2.4. Análisis bivariado y de correlación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5LjDQW5pnrF"
   },
   "source": [
    "Gracias a que nuestro dataset posee solo 9 variables, es posible realizar una mayor cantidad de comparaciones entre variables sin perder fácilmente el control ni el enfoque del análisis exploratorio. En la presente sección exploraremos los datos en búsqueda de patrones de relación directa entre pares de variables bien sea en forma de correlación (linealidad) o, patrones no lineales como clusters.\n",
    "\n",
    "Para empezar, realicemos un pairplot tipo scatter donde vamos a tomar como el criterio de agrupación de datos (etiquetado) las variables más importantes con base a su relación con el número de clientes o alquileres efectuados: la temporada y el tipo de clima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "JNI7CBZKpRlO",
    "outputId": "4f635eb8-6a57-41cd-e933-457a53891d77"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.pairplot(data=df, hue='season', kind='scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZ4zJrVwvDe6"
   },
   "source": [
    "A partir de la observación del pairplot anterior, podemos hacernos una perspectiva general de como los posibles pares de variables expresan patrones dentro de los datos. Podemos observar lo siguiente:\n",
    "* Ningún scatter plot entre variables refleja una relación lineal o de correlación entre sí a escepción de las variables 'temperatura' y 'sensación térmica', lo cuál es natural debido a que éstas dos variables están estrechamente relacionadas y son dependientes entre sí.\n",
    "\n",
    "* Con base al pairplot cuya variable de etiquetado es la temporada, es posible evidenciar patrones tipo cluster, es decir, éste etiquetado permite descubrir que los datos se agrupan y se evidencian cumulos.\n",
    "  * A partir de la relación `cnt-temp`, los alquileres de bicicletas que corresponden a los clientes que alquilan durante el invierno constituyen clientes que alquilan de manera menos frecuente. Éstos clientes de baja frecuencia alquilan en temperaturas bajas.\n",
    "\n",
    "  * A partir de la relación `cnt-temp`, los alquileres de bicicletas que corresponden a los clientes que alquilan durante el otoño constituyen clientes que alquilan con un amplio rango de frecuencia, desde baja hasta alta. Éstos clientes alquilan en temperaturas intermedias.\n",
    "\n",
    "  * A partir de la relación `cnt-temp`, los alquileres de bicicletas que corresponden a los clientes que alquilan durante el verano constituyen clientes que alquilan con un amplio rango de frecuencia, desde baja hasta alta. Éstos clientes alquilan en temperaturas altas.\n",
    "\n",
    "  * A partir de la relación `cnt-temp`, los alquileres de bicicletas que corresponden a los clientes que alquilan durante la primavera no exiben un patron visualmente perceptible, aparentemente se encuentran a frecuencias de alquiler y temperaturas altas.\n",
    "\n",
    "  * De manera general con respecto a la temporada, es posible ver que esta variable estrecha el rango de frecuencia de alquiler de los clientes.\n",
    "\n",
    "  * A partir de la relación `cnt-windspeed`, no es muy claro el patrón de los datos, sin embargo puede insinuarse que a medida que la temporada cambia, la frecuencia de alquiler de los clientes también cambia, donde el invierno refleja los clientes de baja frecuencia, el otoño aquellos de frecuencia intermedia-alta, mientras que el verano y la primavera parecen reflejar clientes desde baja a alta frecuencia.\n",
    "\n",
    "* La relación `hum-temp` ilustra que en general, para las temporadas de invierno, otoño y verano, los alquileres ocurren dentro de un rango de humedad que va desde la humedad baja hasta la alta. Por otra parte los clientes que alquilan en primavera suelen hacerlo en un rango de temperatura amplio  pero éstos clientes alquilan en valores de humedad bajos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "c4EtcAyGtPF9",
    "outputId": "a258ed2f-4a81-405d-bd70-b8f17ef9045b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.pairplot(data=df, hue='weathersit', kind='scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T72H_iWK2T61"
   },
   "source": [
    "A partir del pairplot anterior, es difícil extrar comportamientos o patrones, no obstante la predominancia del color azul deja en evidencia que es en el clima despejado en donde más clientes alquilan bicicletas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Y1NcU8UTub1t",
    "outputId": "6bc89fe2-d5d7-44ba-f93f-42babd72da79"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.pairplot(data=df, hue='time_of_day', kind='scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tckapUgr2zkP"
   },
   "source": [
    "Del anterior pairplot no es posible extraer fácilmente los patrones implicitos en los datos con respecto al tiempo del dia. No obstante puede señalarse lo siguiente:\n",
    "\n",
    "* A partir de la relación `cnt-temp`, los alquileres de bicicletas que corresponden a los clientes que alquilan durante la tarde constituyen clientes que alquilan con un amplio rango de frecuencia, desde baja hasta alta pero sus alquileres ocurren mayormente entorno a temperaturas medias y altas.\n",
    "\n",
    "* A partir de la relación `cnt-temp`, los alquileres de bicicletas que corresponden a los clientes que alquilan durante la mañana constituyen clientes que alquilan con un amplio rango de frecuencia, desde baja hasta alta pero sus alquileres ocurren mayormente entorno a temperaturas medias.\n",
    "\n",
    "* A partir de la relación `cnt-hum`, los alquileres de bicicletas que corresponden a los clientes que alquilan durante la tarde constituyen clientes que alquilan con un amplio rango de frecuencia, desde baja hasta alta pero sus alquileres ocurren mayormente a humedades bajas.\n",
    "\n",
    "* A partir de la relación `cnt-hum`, los alquileres de bicicletas que corresponden a los clientes que alquilan durante la tarde constituyen clientes que alquilan con un amplio rango de frecuencia, desde baja hasta alta pero sus alquileres ocurren mayormente a humedades altas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYgTUVlk7rR4"
   },
   "source": [
    "Para evidenciar cuantitativamente la falta de correlación o colinealidad entre las variables, veremos a continuación que las únicas variables que presentan correlación son la temperatura y la sensación térmica de la temperatura. Por otro lado todas las demás variables poseen un coeficiente de correlación prácticamente nulo. No obstante note que las variables de temperatura, sensación térmica y cantidad de  alquiler por cliente (`'cnt'`) tienen una leve correlación, lo cual insinúa una vez más la relación entre los factores del clima y la propension a alquilar bicicletas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "JnhGvGkU7A21",
    "outputId": "3fa1f723-7888-4e00-a00c-284af616d531"
   },
   "outputs": [],
   "source": [
    "corr_mat = df.corr()\n",
    "sns.heatmap(corr_mat, annot=True, cmap='coolwarm', linewidths=5,\n",
    "            linecolor='black', vmin=-0.5, vmax=1,\n",
    "            cbar=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_tywNAQ68dc"
   },
   "source": [
    "Finalmente, realizaremos un scatter plot que ayude a relacionar múltiples variables relacionadas con el clima tales como la temperatura, la temporada, el tipo de clima y el periodo del dia.\n",
    "\n",
    "No obstante debido a la complejidad de los datos, solo puede concluirse que se evidencia una predominancia de clientes para el clima despejado, donde entre mayor sensación térmica máyor es la presencia de clientes de alta frecuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "XhGPdZSu4P7r",
    "outputId": "537169d6-a7fe-4549-868b-e5358afc7cb3"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "markers = {'Winter':'D','Fall':'s', 'Summer':'*', 'Spring':'<'}\n",
    "sns.relplot(data=df,x='atemp',y='cnt',\n",
    "            hue='weathersit',style='season',size='time_of_day',\n",
    "            markers=markers,\n",
    "            kind='scatter',col='season')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuwMmFXmovoa"
   },
   "source": [
    "### 2.5. Análisis multivariado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D58wlIcQF4Q4"
   },
   "source": [
    "Ahora nos preguntamos, ¿Que tendencia exíste entre el número de regístros, el dia de la semana y el tiempo de la semana?. La respuesta a ésta pregúnta es que en cuanto al número de regístros (no al de número de alquileres por persona) se observa  que la noche es aquella que más regístros presenta, seguido de la mañana y la tarde respectivamente para cada día de la semana. No obstante, es importante resaltar que a pesar de que la tarde tenga menos regístros, aquellas personas que rentan en este periodo del día son personas con una alta constancia de alquiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "_Cl3ls1k2fm2",
    "outputId": "58043de4-1720-450d-b1e9-f5d788b88ee6"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "sns.countplot(data=df, x='weekday', hue='time_of_day')\n",
    "plt.title('Número de clientes según el día y tiempo')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlJOi0l4Gcdv"
   },
   "source": [
    "¿Que patrón existe entre el número de regístros (número de personas) de alquiler, el tipo de clima y el periodo del día?. La respuesta a esta semana es que en general, el tipo de clima afecta fuertemente el número de personas que alquilan donde la jerarquía de manera descendente se da de la forma despejado, nuboso, lluvia ligera y lluvia fuerte respectivamente. Por otra parte se observa que cuando el clíma es despejado, la jerarquía con respecto al periodo del dia es en orden descendente noche, mañana y tarde. Para clima nuboso es mañana, noche y tarde. Para lluvia ligera es noche, tarde y mañana. Para finalmente en lluvia fuerte no presentar una jerarquía notable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "HK056uT7Apzt",
    "outputId": "38bc3650-cf37-47fb-c1e6-19636924ccad"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "sns.countplot(data=df, x='weathersit', hue='time_of_day')\n",
    "plt.title('Número de clientes según el clima y tiempo')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZBG3jL2FIyW",
    "outputId": "b2468de1-7b43-40f5-c50c-608748257bac"
   },
   "outputs": [],
   "source": [
    "df['weathersit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df4gqLt9lsZ2"
   },
   "source": [
    "Si separamos los datos dependiendo del tipo de clima, se evidencia que este es tal vez el factor más importante en cuanto al alquiler de bicicletas. Al realizar un boxplot para cada periodo o tiempo del día según el tipo de clima, se observa que entre más húmedo-lluvioso sea el clima menor cantidad de clientes desearán alquilar bicicletas, así, las distribuciones de frecuencia de alquiler  por dia de la semana para cada clima nos permite afirmar:\n",
    "\n",
    "* El clima en donde mayor cantidad alquileres de bicicletas ocurren es en el clima despejado. Es en este clima en donde exíste una mayor variedad de rangos de frecuencia de alquiler por bliente. En general, entre menos ideal sea el clima (menos despejado) más estrechas serán las distribuciones de frecuencia de alquiler.\n",
    "\n",
    "* El alquiler de bicicletas en dias extremadamente lluviosos es casi nulo.\n",
    "\n",
    "* Sin importar el tipo de clima, se evidencia en general que los clientes que alquilan en la noche son clientes de baja frecuencia de alquiler, los clientes que alquilan en la mañana son clientes de frecuencias intermedias de alquiler mientras que aquellos clientes que alquilan en la tarde son clientes de alta frecuencia de alquiler.\n",
    "\n",
    "* A primera vista, el dia de la semana no es un factor tan importante en el alquiler de bicicletas ya que los patrones de los boxplots permanecen prácticamente iguales. No obstante en climas nublados, se evidencia levemente que los días de mayor acumulación de clientes de alta frecuencia de alquiler son los días 1 y 7 de la semana mientras que los dias 3 y 4 son los días de menos acumulación de éste tipo de clientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "BHK86oE2lA1D",
    "outputId": "0f7cc9a7-4251-4434-9b44-c4eec872f067"
   },
   "outputs": [],
   "source": [
    "sns.catplot(data=df,x='weekday',y='cnt',\n",
    "            hue='time_of_day',dodge=True,kind='box',\n",
    "            col='weathersit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So0yXmVVZSGO"
   },
   "source": [
    "Por otra parte, si ahora separamos los datos de acuerdo a la temporada del año, nos esposible observar lo siguiente:\n",
    "\n",
    "* Sin importar el dia de la semana ni la temporada, los clientes que alquilan bicicletas en la noche son clientes de baja frecuencia de alquiler, aquellos que alquilan en la mañana son clientes de frecuencias de alquiler intermedias mientras que aquellos clientes que alquilan en la tarde son clientes de alta frecuencia.\n",
    "\n",
    "* No se evidencia una dependencia fuerte entre la frecuencia de alquiler por cliente y el día de la semana.\n",
    "\n",
    "* Es en Verano y primavera en donde las distribuciones de frecuencia de alquiler por cliente se hacen más anchas, es decir, los clientes poseen un rango de frecuencia de alquiler más amplio, siendo el verano la temporada en donde los clientes suelen tener una frecuencia de alquiler mayor que en otras temporadas incluso en horarios nocturnos.\n",
    "\n",
    "* El invierno es la temporada en donde el rango de la frecuencia de alquiler es más estrecho sin importar el horario donde tiene lugar el alquilar. En general las frecuencias de alquiler por cliente para esta temporada las más bajas, seguido del otoño, primavera y luego el verano. Siendo el verano donde los clientes adquieren una mayor frecuencia de alquiler.\n",
    "\n",
    "* Sin imporatar el día de la semana o la temporada, la noche es el periodo del día donde se regístran clientes con más baja frecuencia de alquiler, seguido por la mañana y la tarde, siendo la tarde el periodo del día en donde más clientes de alta frecuencia se regístran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "4ur91iRdmII8",
    "outputId": "03c9f60e-08d6-4869-b47d-c74927112122"
   },
   "outputs": [],
   "source": [
    "sns.catplot(data=df,x='weekday',y='cnt',\n",
    "            hue='time_of_day',dodge=True,kind='box',\n",
    "            col='season')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4DEHzAed_iK"
   },
   "source": [
    "Para concluir nuestro análisis exploratorio de los datos, observemos en detalle cómo las variables relacionadas con el clima y el periodo del día afectan las distribuciones de frecuencia de alquiler por cliente.\n",
    "\n",
    "Para comenzar, al hacer un gráfico  de la frecuencia de alquiler por cliente según el clima es posible afirmar:\n",
    "\n",
    "* El tipo de clima tiene un impacto drástico en la cantidad de alquileres y en el tipo de clientes que alquilan bicicletas (en escencia, en la frecuencia de aqluiler de los clientes), de modo tal de que entre más despejado u óptimo sea el clima, se presentan mayor cantidad de clientes (es decir, mayor cantidad de regístros) y mayor frecuencia de alquiler (es decir, mayor cantidad de alquileres por cliente). El efecto del clima es tan determinante, que la suma de todos los alquileres hechos por todos los clientes por cada tipo de clima, resulta ser de tal forma que la cantidad total de alquileres hecha en el clima despejado representa el 70% de todos los alquileres hechos durante el año. Porfavor observe los siguientes dos gráficos los cuales ilustran lo aquí expuesto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "sgH0q3VBHdee",
    "outputId": "61865238-fc80-47cd-c2c2-f8338680a211"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "sns.displot(data=df,x='cnt',hue='weathersit',\n",
    "            stat='count',multiple='stack',kde=True)\n",
    "plt.title('Distribuciones de alquiler según el clima')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0cm4xUPeeRP6",
    "outputId": "7f84c883-1f61-468e-b668-0cb153fd81be"
   },
   "outputs": [],
   "source": [
    "df['weathersit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "VuvI6NWQfIJ9",
    "outputId": "85456ce2-8716-46a6-f315-49836613bc46"
   },
   "outputs": [],
   "source": [
    "grupos_clima = df.groupby('weathersit')['cnt'].sum()\n",
    "porcentaje_clima =(grupos_clima / grupos_clima.values.sum()) *100\n",
    "porcentaje_clima.plot(kind='bar')\n",
    "plt.title('% de suma de alquileres (frecuencia) por cliente según el clima')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Clima')\n",
    "plt.ylabel('% de suma total de los alquileres por cliente')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXdy7U6Uh1_V"
   },
   "source": [
    "Por otra parte, si se realiza una distribución tipo histograma de la frecuencia de alquiler (cantidad de alquileres por cliente) para cada temporada, se puede concluir lo siguiente:\n",
    "\n",
    "* La temporada, al igual que la calidad del clima, es un factor fundamental para el alquiler de bicicletas. Se observa que a medida que la temporada es mas fría, la frecuencia de alquiler (cantidad de alquileres por cliente) disminuye. Note en los siguientes dos gráficos y en el conteo de regístros por temporada, que si bien cada temporada tiene un número de regístros equiparable (todos rondando los 4000), la cantidad de éstos regístros que pertenecen a temporadas más bajas, acumulan sus regístros en las frecuencias de alquiler bajas. Ésto significa que la frecuencia de alquiler es proporcional tanto a la calidad del clima como a la \"calidez\" de la temporada.\n",
    "\n",
    "* La razón de que la distribución para la temporada invierno parezca \"superior\" a la de verano, es que la distribución de verano concentrará más de sus regístros en la cola de la distribución es decir, en los valores de frecuencia de alquiler altas mientras que el invierno concentrará la mayodría de sus regístros en la primera parte de la distribución, es decir, en frecuencias bajas. No obstante, note que según este argumento, el orden de las distribuciónes según su concentración de regístros en frecuencias bajas sería \"invierno, otoño, primavera, verano\", sin embargo, la distribución más \"inferior\" resulta ser la distribución de otoño. Se necesita más análisis en este sentido para determinar el por qué la distribución de otoño se encuentra por debajo de todas las demás distribuciones.\n",
    "\n",
    "* Se evidencia que si sumamos la cantidad de alquileres hechos por todos los clientes en cada temporada (sumamos todas las frecuencias de alquiler), nos damos cuenta de que si organizamos de menor a mayor las temporadas según ésta suma obtenemos el orden \"invierno, otoño, primavera, verano\". Lo anterior nos indica que efectivamente, la cantidad de alquileres es proporcional a la calidez de la temporada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "DD78fG_imgZu",
    "outputId": "25a8a247-39e6-4598-df45-664967112a95"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "sns.displot(data=df,x='cnt',hue='season',\n",
    "            stat='count',multiple='stack',kde=True)\n",
    "plt.title('Distribuciones de alquiler según la temporada')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8svwAHaIdcBF",
    "outputId": "cc57f860-d05e-41c9-a918-70e55a6ed818"
   },
   "outputs": [],
   "source": [
    "df['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "X3Pp-2xqhgy5",
    "outputId": "f2bc89c4-3c2f-4057-9316-03e5e98590f4"
   },
   "outputs": [],
   "source": [
    "grupos_temporada = df.groupby('season')['cnt'].sum()\n",
    "porcentaje_temporada =(grupos_temporada / grupos_temporada.values.sum()) *100\n",
    "porcentaje_temporada.plot(kind='bar')\n",
    "plt.title('% de suma de alquileres (frecuencia) por cliente según la temporada')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Temporada')\n",
    "plt.ylabel('% de suma total de los alquileres por cliente')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_kv7CTogwQ1"
   },
   "source": [
    "Finalmente, si estudiamos la cantidad de regístros y el número total de alquileres por periodo del día, concluimos:\n",
    "\n",
    "* El periodo del día es importante en la cantidad de alquileres, puede ser tanto por la calidez del día como por cuestiones como la cantidad de movimiento de la ciudad. Se evidencia que entre la jerarquía de las distribuciones corresponde a noche, mañana y tarde. Donde la distribución se ubica más hacia la derecha a medida que lso regístros se acumulan más en la región de frecuencias de alquiler altas, note entonces que ésto evidencia que la tarde es el periodo del día donde más alquileres ocurren y clientes de mayor frecuencia tienen lugar, mientras que la noche es el periodo donde los clientes de baja frecuencia tienen lugar.\n",
    "\n",
    "* Para complementar lo anterior, observe que la suma de todos los alquileres efectuados por periodo del día es mayor para la tarde, seguido por la mañana y luego por la noche, lo que convierte a la noche en el periodo de menor cantidad de alquileres hechos en la ciudad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "-3QGTI7SCPmH",
    "outputId": "207ad903-9fe7-4f18-ca7e-d446e89e4745"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "sns.displot(data=df,x='cnt',hue='time_of_day',\n",
    "            stat='count',multiple='stack',kde=True)\n",
    "plt.title('Distribuciones de alquiler según la temporada')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "IkMF_WeycAPL",
    "outputId": "f3bf26d3-50d7-4d79-e06f-a1f161696b03"
   },
   "outputs": [],
   "source": [
    "sns.displot(data=df, x='cnt',\n",
    "            kde=True,\n",
    "            color='blue', rug=True,\n",
    "            col='time_of_day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96YaeWMucWIp",
    "outputId": "efc4aa89-821a-478b-ca92-1e58702cf38b"
   },
   "outputs": [],
   "source": [
    "df['time_of_day'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "QwFIFcDfggD9",
    "outputId": "e72df01b-2ae4-40e8-e1ff-eba8e331fc5e"
   },
   "outputs": [],
   "source": [
    "grupos_temporada = df.groupby('time_of_day')['cnt'].sum()\n",
    "porcentaje_temporada =(grupos_temporada / grupos_temporada.values.sum()) *100\n",
    "porcentaje_temporada.plot(kind='bar')\n",
    "plt.title('% de suma de alquileres (frecuencia) por cliente según la periodo del día')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Periodo del día')\n",
    "plt.ylabel('% de suma total de los alquileres por cliente')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fly1NKiNQrXX"
   },
   "source": [
    "# **3.Modelamiento de los datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGIIZubdbtOc"
   },
   "source": [
    "### 3.1. Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqlRYfaHbwDt"
   },
   "source": [
    "---\n",
    "Preparación de los datos\n",
    "\n",
    "---\n",
    "\n",
    "La presente subsección, desarrolla los diferentes métodos de preprocesamiento de los datos cuyos procedimientos están destinados a preparar los datos para la fase de modelamiento. Dentro de los temas a tratar aquí, hablaremos de las modificaciones y transformaciones efectuadas a los datos con base a las conclusiones desarrolladas en la fase EDA y al conocimiento del contexto del problema.\n",
    "\n",
    "Dentro de la presente sección, encapsularemos todos los procedimientos necesarios en constructores o clases que permitan de manera modular, usar todo un conjunto de funciones de preprocesamiento con el fín de modularizar las partes del código. En general, los procesos a efectuar dentro de la presente subsección son:\n",
    "\n",
    "1. Eliminación de duplicados.\n",
    "2. Escalamiento de los datos.\n",
    "3. Creación de características polinómicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3loh9NcicyV"
   },
   "source": [
    "Antes de empezar, realicemos una copia del dataframe y asegurémonos de que no existan duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oi3a_ZCKiiHd",
    "outputId": "0d056eca-d0ac-492d-85d9-830bef77d22d"
   },
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "data.drop_duplicates()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXY3UYCQjYZ1"
   },
   "source": [
    "Una vez hemos limpiado los datos, a continuación se muestra el constructor usado para realizar los siguientes procedimientos relacionados con el preprocesamiento de los datos y su transformación para posterior modelamiento:\n",
    "\n",
    "* Estandarización de los datos.\n",
    "* Codificación ordinal.\n",
    "* Separación de los datos  en conjuntos de entrenamiento, validación y prueba.\n",
    "\n",
    "Las funciones que componen al constructor de preprocesamiento son:\n",
    "\n",
    "1. `data_scaling`: Función empleada para inicializar un estandarizador tipo `StandardScaler` y ajustar dicho escalador a los datos de entrenamiento (train).\n",
    "\n",
    "2. `data_scaling_fit`: Función para transformar los datos de entrada, tanto aquellos correspondientes a las variables independientes como las dependientes, según la estandarización deducida por la función `data_scaling`.\n",
    "\n",
    "3. `inverse_scaling`: Función para aplicar la transformación inversa a la estandarización aprendida a partir de los datos de entrenamiento (train) para regresar a los datos a su escala original.\n",
    "\n",
    "4. `data_split`: Función para dividir los datos en conjuntos de entrenamiento (train), validación (validation) y prueba (test).\n",
    "\n",
    "5. `ordinal_encoder`: Función para transformar los datos categóricos en datos numéricos ordinales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGeTtixlZDP0"
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "#-------------- Constructor para el procesamiento de los datos ------------------\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "class data_preprocessing:\n",
    "  def __init__(self):\n",
    "    self.x_scaler = None\n",
    "    self.y_scaler = None\n",
    "    self.encoder  = None\n",
    "\n",
    "  #------------------------\n",
    "  # Estandarizador de datos\n",
    "  #------------------------\n",
    "\n",
    "  def data_scaling(self, X_train, y_train):\n",
    "    self.x_scaler  = StandardScaler()\n",
    "    self.y_scaler  = StandardScaler()\n",
    "    x_scaled_train = self.x_scaler.fit_transform(X_train)\n",
    "    y_scaled_train = self.y_scaler.fit_transform(y_train)\n",
    "    return x_scaled_train, y_scaled_train\n",
    "\n",
    "  #--------------------------------\n",
    "  # Aplicador de la estandarización\n",
    "  #--------------------------------\n",
    "\n",
    "  def data_scaling_transform(self, X_data, y_data):\n",
    "    x_scaled_data = 0\n",
    "    y_scaled_data = 0\n",
    "    if (self.x_scaler == None and self.y_scaler == None):\n",
    "      print('First, train the scaler, use the  .data_scaling() method.')\n",
    "    else:\n",
    "      x_scaled_data = self.x_scaler.transform(X_data)\n",
    "      y_scaled_data = self.y_scaler.transform(y_data)\n",
    "    return x_scaled_data, y_scaled_data\n",
    "\n",
    "  #------------------------------------------\n",
    "  # Transformación de estandarización inversa\n",
    "  #------------------------------------------\n",
    "\n",
    "  def inverse_scaling(self, X_scaled_data, y_scaled_data):\n",
    "    if (self.x_scaler == None and self.y_scaler == None):\n",
    "      print('First, train the scaler, use the  .data_scaling() method.')\n",
    "    else:\n",
    "      x_original_data = self.x_scaler.inverse_transform(X_scaled_data)\n",
    "      y_original_data = self.y_scaler.inverse_transform(y_scaled_data)\n",
    "    return x_original_data, y_original_data\n",
    "\n",
    "  #---------------------\n",
    "  # Divisor de los datos\n",
    "  #---------------------\n",
    "\n",
    "  def split(self, X_data, y_data, train_size, test_size, validation=False,val_size=0):\n",
    "    random_state = 77\n",
    "    if validation:\n",
    "      X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, random_state = random_state,\n",
    "                                                        shuffle=True, test_size=val_size)\n",
    "      X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, random_state = random_state,\n",
    "                                                        shuffle=True, test_size=test_size)\n",
    "      return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    else:\n",
    "      X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, random_state = random_state,\n",
    "                                                        shuffle=True, test_size=test_size)\n",
    "      return X_train, X_test, y_train, y_test\n",
    "\n",
    "  #----------------\n",
    "  # Ordinal encoder\n",
    "  #----------------\n",
    "\n",
    "  def ordinal_encoder(self, X_data, inverse=False):\n",
    "    df = X_data.copy()\n",
    "    categorical     = [columna for columna in df.columns if data[columna].dtype == object]\n",
    "    non_categorical = [columna for columna in df.columns if data[columna].dtype != object]\n",
    "    if not inverse:\n",
    "      self.encoder  = OrdinalEncoder()\n",
    "      X_encoded     = self.encoder.fit_transform(df[categorical])\n",
    "    else:\n",
    "      X_encoded     = self.encoder.inverse_transform(df[categorical])\n",
    "\n",
    "    X_encoded = pd.DataFrame(X_encoded, columns=categorical, index=df.index)\n",
    "    X_final = pd.concat([df[non_categorical], X_encoded], axis=1)\n",
    "    return X_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6ofjC6PSjm8"
   },
   "source": [
    "### 3.2. Modelamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaRy12emSpeQ"
   },
   "source": [
    "---\n",
    "Modelamiento de los datos\n",
    "\n",
    "---\n",
    "\n",
    "La presente subsección está destinada a realizar el modelamiento de los datos mediante dos modelos: la regresión polinomial y la regresión lineal regularizada Lasso. Verémos a continuación, que dentro de los procedimientos de preparación de datos no se ha decidido eliminar los valores atípicos de ninguna e las variables, ya que se considera que éstos valores atípicos no son lo demasiado extremos deacuerdo a ninguna de las variables analizadas dentro de la fase EDA para justificar una necesidad imperante de removerlos de la fase de modelamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlvTrvR-PLzQ"
   },
   "source": [
    "##### 3.2.1. Modelo polinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49WIZWuot7s8"
   },
   "source": [
    "---\n",
    "Regresión polinomial\n",
    "\n",
    "---\n",
    "\n",
    "En esta subsección, para realizar el modelamiento de los datos, hemos seleccionado un modelo de regresión polinomial. A continuación, se realizan los siguientes procedimientos:\n",
    "\n",
    "* Preprocesamiento de los datos:\n",
    "\n",
    "  * División de los datos en variables independientes `X` y variables dependientes `y`.\n",
    "\n",
    "  * Codificación ordinal. Ésto se hace para transformar los datos categóricos a numéricos ordinales. Hemos preferido adoptar un Ordinal Encoder en lugar de un OneHotEncoder ya que deseamos preservar el mísmo número de columnas en los datos de entrada para evitar un tiempo de entrenamiento más largo. Tambíen, hemos preferido este tipo de codificación para poder observar más adelante la importancia de las variables que conforman al conjunto de datos con respecto a la predicción del modelo, y no la importancia de las clases independientes.\n",
    "\n",
    "  * División de los datos en los conjuntos de entrenamiento, validación y testeo.\n",
    "\n",
    "  * Estandarización de los datos. Realizamos una estandarización de los datos de entrada `X` y `y` con base a los conjuntos de entrenamiento `X_train` y  `y_train` para asegurar una convergencia más rápida del algoritmo y una mayor presición en la predicción de la variable `y`. El objeto de estandarización, aprende los parámetros de la transformación a partir del conjunto de entrenamiento, luego, aplica ésta transformación a los conjuntos de valiración y prueba independientemente.\n",
    "\n",
    "* Modelamiento de los datos por regresión polinomial:\n",
    "  * Creamos un regresor polinomial mediante un pipeline compuesto por características polinomiales, RobustScaler y regresor lineal Lasso. El generador de características polinomiales genera las columnas dentro del dataset correspondientes a las diferentes combinaciones polinomiales de las características originales de la data. Posteriormente, debido a que éstas combinaciones implican potencias y productos entre características, el RobustScaler ejecuta una estandarización robusta (no suceptible a valores atípicos) a cada característica polinomial, ésto se hace para evitar que los valores de las características adquieran valores muy grandes impidiento la precisa y rápida convergencia del regresor. Finalmente, con los datos convertidos a características polinomiales estandarizdas, un regresor lineal regularizado por Lasso ejecuta la regresión.\n",
    "\n",
    "  * El entrenamiento del modelo se ejecuta mediante una búsqueda de parámetros usando la técnica `GridSearch`, la cuál es una técnica para encontrar los mejores hiperparámetros de un modelo, probando exhaustivamente diferentes combinaciones de valores predefinidos en una cuadrícula. Los puntajes de éste entrenamiento son calculados mediante un paradigma de Cross Vlidation.\n",
    "\n",
    "  * Finalmente, se aplica la transformación de estandarización inversa para retornar a los datos a su escala original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B30uIWryDNQ9"
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------\n",
    "# Preprocesamiento de los datos de entrada\n",
    "#-----------------------------------------\n",
    "\n",
    "#----Division de los datos en las variables independientes X y la variable objetivo y---\n",
    "X_data = data.drop(['cnt'], axis=1)\n",
    "y_data = data[['cnt']]\n",
    "\n",
    "#----Ordinal encoding----\n",
    "data_processor = data_preprocessing()\n",
    "X_encoded = data_processor.ordinal_encoder(X_data, inverse=False)\n",
    "\n",
    "#----Data splitting----\n",
    "train_size     = 0.6\n",
    "val_test_size  = 0.4\n",
    "\n",
    "X_train, X_dev  , y_train, y_dev  = data_processor.split( X_encoded, y_data, train_size, val_test_size )\n",
    "X_val  , X_test , y_val  , y_test = data_processor.split( X_dev, y_dev, 0.5, 0.5 )\n",
    "\n",
    "#----Data standarization----\n",
    "X_train_scaled, y_train_scaled    = data_processor.data_scaling( X_train, y_train )\n",
    "X_val_scaled  , y_val_scaled      = data_processor.data_scaling_transform( X_val, y_val )\n",
    "X_test_scaled , y_test_scaled     = data_processor.data_scaling_transform( X_test, y_test )\n",
    "\n",
    "#---------------------------------------------------\n",
    "# Modelamiento de los datos por regresión polinomial\n",
    "#---------------------------------------------------\n",
    "\n",
    "#----Polynomial regressor----\n",
    "polynomial_regression = make_pipeline(PolynomialFeatures(),\n",
    "                                      RobustScaler(),\n",
    "                                      Lasso()\n",
    "                                      )\n",
    "\n",
    "#----Método de ensamble bagging----\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': [2, 3],\n",
    "              'lasso__alpha':[0.001, 0.01,0.1,1]}\n",
    "\n",
    "\n",
    "#----Entrenamiento del modelo por GridSearch con validación cruzada----\n",
    "cantidad_de_folds = 10\n",
    "kfold = KFold(n_splits=cantidad_de_folds, shuffle=True, random_state = 77)\n",
    "grid  = GridSearchCV(polynomial_regression, param_grid, cv=kfold, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "grid.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "best_model  = grid.best_estimator_\n",
    "best_params = grid.best_params_\n",
    "\n",
    "y_train_pred = best_model.predict(X_train_scaled)\n",
    "y_val_pred   = best_model.predict(X_val_scaled)\n",
    "y_test_pred  = best_model.predict(X_test_scaled)\n",
    "\n",
    "X_train_org, y_train_pred  = data_processor.inverse_scaling(X_train_scaled, y_train_pred.reshape(-1,1))\n",
    "X_val_org  , y_val_pred    = data_processor.inverse_scaling(X_val_scaled, y_val_pred.reshape(-1,1))\n",
    "X_test_org , y_test_pred   = data_processor.inverse_scaling(X_test_scaled, y_test_pred.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMCfcLLoqRUu"
   },
   "source": [
    "Una vez hemos entrenado nuestro modelo y seleccionado los mejores parámetros para el modelo polinomial concernientes a su grado y regularización, usaremos éstos mejores parámetros encontrados por una búsqueda exaustiva usando validación cruzada para re entrenar el modelo pero esta vez, sobre el conjunto de datos completo. Para esta labor, a continuación se realizan los siguientes procedimientos:\n",
    "\n",
    "* Preprocesamiento de los datos:\n",
    "  * Permutación aleatoria del conjunto de datos completo. Ésto se hace solo para prevenir un posible sobreajuste.\n",
    "\n",
    "  * División de los datos en variables independientes `X` y variables dependientes `y`.\n",
    "\n",
    "  * Codificación ordinal. Ésto se hace para transformar los datos categóricos a numéricos ordinales. Hemos preferido adoptar un Ordinal Encoder en lugar de un OneHotEncoder ya que deseamos preservar el mísmo número de columnas en los datos de entrada para evitar un tiempo de entrenamiento más largo. Tambíen, hemos preferido este tipo de codificación para poder observar más adelante la importancia de las variables que conforman al conjunto de datos con respecto a la predicción del modelo, y no la importancia de las clases independientes.\n",
    "\n",
    "  * Estandarización de los datos. Realizamos una estandarización de los datos de entrada `X` y `y` para asegurar una convergencia más rápida del algoritmo y una mayor presición en la predicción de la variable `y`.\n",
    "\n",
    "* Modelamiento de los datos por regresión polinomial:\n",
    "  * Creamos un regresor polinomial mediante un pipeline compuesto por características polinomiales, RobustScaler y regresor lineal Lasso. El generador de características polinomiales genera las columnas dentro del dataset correspondientes a las diferentes combinaciones polinomiales de las características originales de la data. Posteriormente, debido a que éstas combinaciones implican potencias y productos entre características, el RobustScaler ejecuta una estandarización robusta (no suceptible a valores atípicos) a cada característica polinomial, ésto se hace para evitar que los valores de las características adquieran valores muy grandes impidiento la precisa y rápida convergencia del regresor. Finalmente, con los datos convertidos a características polinomiales estandarizdas, un regresor lineal regularizado por Lasso ejecuta la regresión.\n",
    "\n",
    "  * El entrenamiento del modelo se ejecuta mediante una técnica de `BaggingRegressor` la cuál es una función que ejecuta un entrenamiento del modelo en cuestión ejecutando primero un entrenamiento Bagging (Bootstrap Aggregating) la cuál es una técnica de conjunto que combina múltiples modelos independientes para reducir la varianza y mejorar la generalización del modelo.\n",
    "\n",
    "  * Finalmente, se aplica la transformación de estandarización inversa para retornar a los datos a su escala original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fh-yaX8VWwpO"
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------\n",
    "# Preprocesamiento de los datos de entrada\n",
    "#-----------------------------------------\n",
    "\n",
    "#----Cambio aleatorio del orden de los regístros del dataset----\n",
    "data_shuffled = data.sample(frac=1, random_state=13)\n",
    "\n",
    "#----Division de los datos en las variables independientes X y la variable objetivo y---\n",
    "X_data_full = data_shuffled.drop(['cnt'], axis=1)\n",
    "y_data_full = data_shuffled[['cnt']]\n",
    "\n",
    "#----Ordinal encoding----\n",
    "data_processor = data_preprocessing()\n",
    "X_encoded = data_processor.ordinal_encoder(X_data_full, inverse=False)\n",
    "\n",
    "#----Data standarization----\n",
    "X_full_scaled, y_full_scaled = data_processor.data_scaling( X_encoded, y_data_full )\n",
    "\n",
    "\n",
    "#---------------------------------------------------\n",
    "# Modelamiento de los datos por regresión polinomial\n",
    "#---------------------------------------------------\n",
    "\n",
    "#----Polynomial regressor----\n",
    "best_model_full = make_pipeline(PolynomialFeatures(degree = best_params['polynomialfeatures__degree']),\n",
    "                               RobustScaler(),\n",
    "                               Lasso(alpha = best_params['lasso__alpha'])\n",
    "                               )\n",
    "\n",
    "#----Método de ensamble bagging----\n",
    "bagg_model_full = BaggingRegressor(best_model_full,\n",
    "                                   n_estimators = 5,\n",
    "                                   max_samples  = 0.7,\n",
    "                                   random_state = 77\n",
    "                                   )\n",
    "\n",
    "#----Entrenamiento por ensamble----\n",
    "bagg_model_full.fit(X_full_scaled, y_full_scaled)\n",
    "best_model_full = min(bagg_model_full.estimators_, key=lambda model: mean_squared_error(y_test, model.predict(X_test)))\n",
    "y_pred_full     = best_model_full.predict(X_full_scaled)\n",
    "\n",
    "#----Entrenamiento por ensamble----\n",
    "best_model_full.fit(X_full_scaled, y_full_scaled)\n",
    "y_pred_full          = best_model_full.predict(X_full_scaled)\n",
    "\n",
    "#----Escalamiento inverso de los datos----\n",
    "X_full_org , y_pred_full_poly   = data_processor.inverse_scaling(X_full_scaled, y_pred_full .reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "l1RGUH8lFcwJ",
    "outputId": "db748841-6442-4562-ddfa-ffc8ca9e851b"
   },
   "outputs": [],
   "source": [
    "# Mejor modelo encontrado por Grid Search\n",
    "best_poly_model = best_model_full\n",
    "best_poly_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ehzxT20ajsV",
    "outputId": "b674a1f4-9084-433f-e83f-023846a20ddd"
   },
   "outputs": [],
   "source": [
    "# Mejores parametros polinomiales\n",
    "poly_best_params = best_params\n",
    "poly_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTNK8Hmnux9c"
   },
   "source": [
    "Para medir el desempeño del modelo y saber si se está incurriendo en alguna clase de sobre o sub ajuste, a continuación se calculan los siguientes puntajes según las métricas de RMSE, MAE y R2:\n",
    "\n",
    "* Desempeño durante la selección de hiperparámetros:\n",
    "  * Puntaje sobre el conjunto train.\n",
    "  * Puntaje del mejor modelo en un paradigma de Cross Validation.\n",
    "  * Puntaje del mejor modelo probeniente del GridSearch sobre el conjunto de validación.\n",
    "  * Puntaje del mejor modelo probeniente del GridSearch sobre el conjunto de prueba.\n",
    "\n",
    "* Desempeño durante el entrenamiento total:\n",
    " * Puntaje del mejor modelo sobre el conjunto total de datos.\n",
    " * Puntaje del mejor modelo sobre el conjunto total de datos en un paradigma de Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "6gpI1Z6hSgF6",
    "outputId": "06a91f50-8df3-4519-e2b6-2316165a95b7"
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "# Validación de desempeño del modelo durante la selección de hiperparámetros\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "#----Rmse, Mae y R2----\n",
    "y_data  = [y_train, y_val, y_test]\n",
    "y_preds = [y_train_pred, y_val_pred, y_test_pred]\n",
    "\n",
    "labels_rmse = ['Rmse Train', 'Rmse Validation', 'Rmse Test', 'Rmse CV', 'Rmse full model', 'Rmse CV full model']\n",
    "labels_mae  = ['Mae Train', 'Mae Validation', 'Mae Test', 'Mae CV', 'Mae full model', 'Mae CV full model']\n",
    "labels_r2   = ['R2 Train', 'R2 Validation', 'R2 Test', 'R2 CV', 'R2 full model', 'R2 CV full model']\n",
    "\n",
    "rmse = [mean_squared_error(y_data[i], y_preds[i],squared=False) for i in range(len(y_preds))]\n",
    "mae  = [mean_absolute_error(y_data[i], y_preds[i]) for i in range(len(y_preds))]\n",
    "r2   = [r2_score(y_data[i], y_preds[i]) for i in range(len(y_preds))]\n",
    "\n",
    "#----puntaje por validación cruzada----\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state = 77)\n",
    "cv_scores_hy     = cross_val_score(best_model, X_train, y_train, cv=kfold, scoring='neg_root_mean_squared_error')\n",
    "re_scorer_hy     = make_scorer(r2_score)\n",
    "cv_scores_r2_hy  = cross_val_score(best_model, X_train, y_train, cv=kfold, scoring=re_scorer_hy)\n",
    "cv_scores_mae_hy = cross_val_score(best_model, X_train, y_train, cv=kfold, scoring=make_scorer(mean_absolute_error))\n",
    "\n",
    "cv_scores_hy    = - cv_scores_hy\n",
    "cv_rmse_mean_hy = cv_scores_hy.mean()\n",
    "cv_mae_mean_hy  = cv_scores_mae_hy.mean()\n",
    "cv_r2_mean_hy   = cv_scores_r2_hy.mean()\n",
    "\n",
    "rmse.append(cv_rmse_mean_hy)\n",
    "mae.append(cv_mae_mean_hy)\n",
    "r2.append(cv_r2_mean_hy)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Validación de desempeño del modelo durante entrenamiento sobre todos los datos\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "#----Rmse y R2----\n",
    "rmse.append( mean_squared_error(y_data_full, y_pred_full_poly, squared=False) )\n",
    "mae.append( mean_absolute_error(y_data_full, y_pred_full_poly) )\n",
    "r2.append( r2_score(y_data_full, y_pred_full_poly) )\n",
    "\n",
    "#----puntaje por validación cruzada----\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state = 77)\n",
    "cv_scores = cross_val_score(best_model_full, X_encoded, y_data_full, cv=kfold, scoring='neg_root_mean_squared_error')\n",
    "re_scorer = make_scorer(r2_score)\n",
    "cv_scores_r2 = cross_val_score(best_model_full, X_encoded, y_data_full, cv=kfold, scoring=re_scorer)\n",
    "cv_scores_mae = cross_val_score(best_model_full, X_encoded, y_data_full, cv=kfold, scoring=make_scorer(mean_absolute_error))\n",
    "\n",
    "cv_scores    = - cv_scores\n",
    "cv_rmse_mean = cv_scores.mean()\n",
    "cv_mae_mean  = cv_scores_mae.mean()\n",
    "cv_r2_mean   = cv_scores_r2.mean()\n",
    "\n",
    "rmse.append(cv_rmse_mean)\n",
    "mae.append(cv_mae_mean)\n",
    "r2.append(cv_r2_mean)\n",
    "\n",
    "#------------------------------\n",
    "# Puntajes generales del modelo\n",
    "#------------------------------\n",
    "poly_stats = pd.DataFrame({'Rmse type' : labels_rmse,\n",
    "                           'Rmse value': rmse,\n",
    "                           'Mae type'  : labels_mae,\n",
    "                           'Mae value' : mae,\n",
    "                           'R2 type'   : labels_r2,\n",
    "                           'R2 value'  : r2})\n",
    "\n",
    "poly_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0lkjJz8zHrL"
   },
   "source": [
    "Para hacernos una idea visual de la calidad de las predicciones de nuestro modelo, graficamos en el eje X los valores reales del conjunto target de prueba y del conjunto de datos en total, mientras que en el eje y, destinamos los valores predichos para la variable target en el conjunto de prueba y en el conjunto de datos total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "zkrVNxMkB8KA",
    "outputId": "5011f5df-a9a2-4cc1-85fb-eb405f48ea3b"
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------\n",
    "# Visualización comparativa de los datos de la variable target y predicha\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "x_test_plot = y_test.values\n",
    "y_test_plot = y_test_pred.reshape(-1,1)\n",
    "\n",
    "x_full_plot = y_data_full.values\n",
    "y_full_plot = y_pred_full_poly.reshape(-1,1)\n",
    "\n",
    "print('Test size = ', x_test_plot.shape)\n",
    "print('Full data size =', x_full_plot.shape)\n",
    "\n",
    "r2_test = round(r2_score(x_test_plot , y_test_plot),4)\n",
    "rmse_test = np.sqrt(mean_squared_error(x_test_plot , y_test_plot, squared=False))\n",
    "r2_full = round(r2_score(x_full_plot, y_full_plot),4)\n",
    "rmse_full = np.sqrt(mean_squared_error(x_full_plot, y_full_plot, squared=False))\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(10,4))\n",
    "ax[0].scatter(x_test_plot , y_test_plot, c='blue', alpha=0.5, label=f'R2: {r2_test:.2f}\\nRMSE: {rmse_test:.2f}')\n",
    "ax[0].plot(x_test_plot,x_test_plot, color='black')\n",
    "ax[0].set_title('Conjunto target test vs predicciones')\n",
    "ax[0].grid(True)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].scatter(x_full_plot,y_full_plot, c='green', alpha=0.5, label=f'R2: {r2_full:.2f}\\nRMSE: {rmse_full:.2f}')\n",
    "ax[1].plot(x_full_plot,x_full_plot, color='black')\n",
    "ax[1].set_title('Variable target vs predicciones')\n",
    "ax[1].grid(True)\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsyG3nYVwRs-"
   },
   "source": [
    "##### 3.2.2. Modelo lineal regularizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ME7U4WyTwU_1"
   },
   "source": [
    "---\n",
    "Regresión Lasso\n",
    "\n",
    "---\n",
    "\n",
    "En esta subsección, para realizar el modelamiento de los datos, hemos seleccionado un modelo de regresión lineal regularizada por mecanísmo Lasso. A continuación, se realizan los siguientes procedimientos:\n",
    "\n",
    "* Preprocesamiento de los datos:\n",
    "\n",
    "  * División de los datos en variables independientes `X` y variables dependientes `y`.\n",
    "\n",
    "  * Codificación ordinal. Ésto se hace para transformar los datos categóricos a numéricos ordinales. Hemos preferido adoptar un Ordinal Encoder en lugar de un OneHotEncoder ya que deseamos preservar el mísmo número de columnas en los datos de entrada para evitar un tiempo de entrenamiento más largo. Tambíen, hemos preferido este tipo de codificación para poder observar más adelante la importancia de las variables que conforman al conjunto de datos con respecto a la predicción del modelo, y no la importancia de las clases independientes.\n",
    "\n",
    "  * División de los datos en los conjuntos de entrenamiento, validación y testeo.\n",
    "\n",
    "  * Estandarización de los datos. Realizamos una estandarización de los datos de entrada `X` y `y` con base a los conjuntos de entrenamiento `X_train` y  `y_train` para asegurar una convergencia más rápida del algoritmo y una mayor presición en la predicción de la variable `y`. El objeto de estandarización, aprende los parámetros de la transformación a partir del conjunto de entrenamiento, luego, aplica ésta transformación a los conjuntos de valiración y prueba independientemente.\n",
    "\n",
    "* Modelamiento de los datos por regresión lineal Lasso:\n",
    "  * Creamos un regresor lineal regularizado mediante el mecanismo Lasso, el cuál, difiere de la regresión lineal simple en que en la función de costo, se adiciona un término correspondiente  a la suma del valor absoluto de los parámetros del modelo.\n",
    "\n",
    "  * El entrenamiento del modelo se ejecuta mediante una técnica que combina `BaggingRegressor` la cuál es una función que ejecuta un entrenamiento del modelo en cuestión ejecutando primero un entrenamiento Bagging (Bootstrap Aggregating) la cuál es una técnica de conjunto que combina múltiples modelos independientes para reducir la varianza y mejorar la generalización del modelo, y una búsqueda de parámetros mediante `GridSearch`, la cuál es una técnica para encontrar los mejores hiperparámetros de un modelo, probando exhaustivamente diferentes combinaciones de valores predefinidos en una cuadrícula.\n",
    "\n",
    "  * Finalmente, se aplica la transformación de estandarización inversa para retornar a los datos a su escala original.\n",
    "\n",
    "\n",
    "No obstante, realizaremos dos evaluaciones de nuestro enfoque, en donde entrenaremos el modelo con datos estandarizados y luego con datos no estandarizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAE74-rOwXiA"
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------\n",
    "# Preprocesamiento de los datos de entrada\n",
    "#-----------------------------------------\n",
    "\n",
    "#----Division de los datos en las variables independientes X y la variable objetivo y---\n",
    "X_data = data.drop(['cnt'], axis=1)\n",
    "y_data = data[['cnt']]\n",
    "\n",
    "#----Ordinal encoding----\n",
    "data_processor = data_preprocessing()\n",
    "X_encoded = data_processor.ordinal_encoder(X_data, inverse=False)\n",
    "\n",
    "#----Data splitting----\n",
    "train_size     = 0.6\n",
    "val_test_size  = 0.4\n",
    "\n",
    "X_train, X_dev  , y_train, y_dev  = data_processor.split( X_encoded, y_data, train_size, val_test_size )\n",
    "X_val  , X_test , y_val  , y_test = data_processor.split( X_dev, y_dev, 0.5, 0.5 )\n",
    "\n",
    "#----Data standarization----\n",
    "X_train_scaled, y_train_scaled    = data_processor.data_scaling( X_train, y_train )\n",
    "X_val_scaled  , y_val_scaled      = data_processor.data_scaling_transform( X_val, y_val )\n",
    "X_test_scaled , y_test_scaled     = data_processor.data_scaling_transform( X_test, y_test )\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "# Modelamiento de los datos por regresión lineal regularizada (para datos estandarizados)\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "#----Regresor Lasso----\n",
    "lasso_regression = Lasso()\n",
    "\n",
    "#----Método de ensamble bagging----\n",
    "bagg_lasso = BaggingRegressor(lasso_regression, random_state=77)\n",
    "param_grid = {'n_estimators':[5,10,15,20],\n",
    "              'max_samples' :[0.5,0.7,1.0],\n",
    "              'estimator__alpha':[0.001, 0.01,0.1, 1, 2, 3, 4, 5]}\n",
    "\n",
    "#----Entrenamiento del modelo por GridSearch con validación cruzada----\n",
    "cantidad_de_folds = 10\n",
    "kfold = KFold(n_splits=cantidad_de_folds, shuffle=True, random_state = 77)\n",
    "grid  = GridSearchCV(bagg_lasso, param_grid, cv=kfold, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "grid.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "best_model  = grid.best_estimator_\n",
    "best_params = grid.best_params_\n",
    "\n",
    "y_train_pred = best_model.predict(X_train_scaled)\n",
    "y_val_pred   = best_model.predict(X_val_scaled)\n",
    "y_test_pred  = best_model.predict(X_test_scaled)\n",
    "\n",
    "X_train_org, y_train_pred  = data_processor.inverse_scaling(X_train_scaled, y_train_pred.reshape(-1,1))\n",
    "X_val_org  , y_val_pred    = data_processor.inverse_scaling(X_val_scaled, y_val_pred.reshape(-1,1))\n",
    "X_test_org , y_test_pred   = data_processor.inverse_scaling(X_test_scaled, y_test_pred.reshape(-1,1))\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# Modelamiento de los datos por regresión lineal regularizada (para datos NO estandarizados)\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "#----Regresor Lasso----\n",
    "lasso_regression = Lasso()\n",
    "\n",
    "#----Método de ensamble bagging----\n",
    "bagg_lasso_nostd = BaggingRegressor(lasso_regression, random_state=77)\n",
    "param_grid = {'n_estimators':[5,10,15,20],\n",
    "              'max_samples' :[0.5,0.7,1.0],\n",
    "              'estimator__alpha':[1, 2, 3, 4, 5]}\n",
    "\n",
    "#----Entrenamiento del modelo por GridSearch con validación cruzada----\n",
    "cantidad_de_folds = 10\n",
    "kfold = KFold(n_splits=cantidad_de_folds, shuffle=True, random_state = 77)\n",
    "grid_nostd  = GridSearchCV(bagg_lasso_nostd, param_grid, cv=kfold, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "grid_nostd.fit(X_train, y_train)\n",
    "\n",
    "best_model_nostd  = grid_nostd.best_estimator_\n",
    "best_params_nostd = grid_nostd.best_params_\n",
    "\n",
    "y_train_pred_nostd = best_model_nostd.predict(X_train)\n",
    "y_val_pred_nostd   = best_model_nostd.predict(X_val)\n",
    "y_test_pred_nostd  = best_model_nostd.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jk4U_gkDxiIr"
   },
   "source": [
    "Una vez hemos entrenado nuestro modelo y seleccionado los mejores parámetros para el modelo Lasso concernientes a regularización, usaremos éstos mejores parámetros encontrados por una búsqueda exaustiva usando validación cruzada para re entrenar el modelo pero esta vez, sobre el conjunto de datos completo. Para esta labor, a continuación se realizan los siguientes procedimientos:\n",
    "\n",
    "* Preprocesamiento de los datos:\n",
    "  * Permutación aleatoria del conjunto de datos completo. Ésto se hace solo para prevenir un posible sobreajuste.\n",
    "\n",
    "  * División de los datos en variables independientes `X` y variables dependientes `y`.\n",
    "\n",
    "  * Codificación ordinal. Ésto se hace para transformar los datos categóricos a numéricos ordinales. Hemos preferido adoptar un Ordinal Encoder en lugar de un OneHotEncoder ya que deseamos preservar el mísmo número de columnas en los datos de entrada para evitar un tiempo de entrenamiento más largo. Tambíen, hemos preferido este tipo de codificación para poder observar más adelante la importancia de las variables que conforman al conjunto de datos con respecto a la predicción del modelo, y no la importancia de las clases independientes.\n",
    "\n",
    "  * Estandarización de los datos. Realizamos una estandarización de los datos de entrada `X` y `y` para asegurar una convergencia más rápida del algoritmo y una mayor presición en la predicción de la variable `y`.\n",
    "\n",
    "* Modelamiento de los datos por regresión polinomial:\n",
    "  * Creamos un regresor lineal regularizado mediante el mecanismo Lasso, el cuál, difiere de la regresión lineal simple en que en la función de costo, se adiciona un término correspondiente  a la suma del valor absoluto de los parámetros del modelo.\n",
    "\n",
    "  * El entrenamiento del modelo se ejecuta mediante una técnica de `BaggingRegressor` la cuál es una función que ejecuta un entrenamiento del modelo en cuestión ejecutando primero un entrenamiento Bagging (Bootstrap Aggregating) la cuál es una técnica de conjunto que combina múltiples modelos independientes para reducir la varianza y mejorar la generalización del modelo.\n",
    "\n",
    "  * Finalmente, se aplica la transformación de estandarización inversa para retornar a los datos a su escala original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vx2q4VkN_Md0",
    "outputId": "330558d8-77a6-44a7-d205-d289f205aebd"
   },
   "outputs": [],
   "source": [
    "#----Cambio aleatorio del orden de los regístros del dataset----\n",
    "data_shuffled = data.sample(frac=1, random_state=13)\n",
    "\n",
    "#----Division de los datos en las variables independientes X y la variable objetivo y---\n",
    "X_data_full = data_shuffled.drop(['cnt'], axis=1)\n",
    "y_data_full = data_shuffled[['cnt']]\n",
    "\n",
    "#----Ordinal encoding----\n",
    "data_processor = data_preprocessing()\n",
    "X_encoded = data_processor.ordinal_encoder(X_data_full, inverse=False)\n",
    "only_linear = Lasso(alpha=5)\n",
    "only_linear.fit(X_encoded, y_data_full)\n",
    "tester = only_linear.predict(X_encoded)\n",
    "\n",
    "print(r2_score(y_data_full, tester))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OozuZkWyx-Fd"
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------\n",
    "# Preprocesamiento de los datos de entrada\n",
    "#-----------------------------------------\n",
    "\n",
    "#----Cambio aleatorio del orden de los regístros del dataset----\n",
    "data_shuffled = data.sample(frac=1, random_state=13)\n",
    "\n",
    "#----Division de los datos en las variables independientes X y la variable objetivo y---\n",
    "X_data_full = data_shuffled.drop(['cnt'], axis=1)\n",
    "y_data_full = data_shuffled[['cnt']]\n",
    "\n",
    "#----Ordinal encoding----\n",
    "data_processor = data_preprocessing()\n",
    "X_encoded = data_processor.ordinal_encoder(X_data_full, inverse=False)\n",
    "\n",
    "#----Data standarization----\n",
    "X_full_scaled, y_full_scaled = data_processor.data_scaling( X_encoded, y_data_full )\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "# Modelamiento de los datos por regresión lineal regularizada (para datos estandarizados)\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "#----Lasso regressor----\n",
    "best_model_full = Lasso(alpha = best_params['estimator__alpha'])\n",
    "\n",
    "#----Método de ensamble bagging----\n",
    "bagg_model_full = BaggingRegressor(best_model_full,\n",
    "                                   n_estimators = best_params['n_estimators'],\n",
    "                                   max_samples  = best_params['max_samples'],\n",
    "                                   random_state = 77\n",
    "                                   )\n",
    "\n",
    "#----Entrenamiento por ensamble----\n",
    "bagg_model_full.fit(X_full_scaled, y_full_scaled)\n",
    "best_bagg_model_full = min(bagg_model_full.estimators_, key=lambda model: mean_squared_error(y_test, model.predict(X_test)))\n",
    "y_pred_full          = best_bagg_model_full.predict(X_full_scaled)\n",
    "\n",
    "#----Escalamiento inverso de los datos----\n",
    "X_full_lin , y_pred_full_lin   = data_processor.inverse_scaling(X_full_scaled, y_pred_full .reshape(-1,1))\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# Modelamiento de los datos por regresión lineal regularizada (para datos NO estandarizados)\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "#----Lasso regressor----\n",
    "best_model_full_nostd = Lasso(alpha = best_params_nostd['estimator__alpha'])\n",
    "\n",
    "#----Método de ensamble bagging----\n",
    "bagg_model_full_nostd = BaggingRegressor(best_model_full_nostd,\n",
    "                                        n_estimators = best_params_nostd['n_estimators'],\n",
    "                                        max_samples  = best_params_nostd['max_samples'],\n",
    "                                        random_state = 77\n",
    "                                        )\n",
    "\n",
    "#----Entrenamiento por ensamble----\n",
    "bagg_model_full_nostd.fit(X_encoded, y_data_full)\n",
    "best_bagg_model_full_nostd = min(bagg_model_full_nostd.estimators_, key=lambda model: mean_squared_error(y_test, model.predict(X_test)))\n",
    "y_pred_full_nostd          = best_bagg_model_full_nostd.predict(X_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "PQ1tIThEVqJp",
    "outputId": "05c3ce02-342a-4e86-ae95-a7c11e498ce0"
   },
   "outputs": [],
   "source": [
    "# Mejor modelo encontrado por Grid Search\n",
    "best_linear_model = best_bagg_model_full\n",
    "best_linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VUaXIiCnYDRk",
    "outputId": "d65c90bf-0c40-48ad-adae-400f4d5c1926"
   },
   "outputs": [],
   "source": [
    "# Mejores parámetros\n",
    "linear_best_params = best_params\n",
    "linear_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "XbDz8Yl8EaBh",
    "outputId": "9da7f7ad-c055-4e0d-fa32-f3540fd11528"
   },
   "outputs": [],
   "source": [
    "# Mejor modelo encontrado por Grid Search para datos NO estandarizados\n",
    "best_linear_model_nostd = best_bagg_model_full_nostd\n",
    "best_linear_model_nostd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WYQQc5SIEe6h",
    "outputId": "6243135c-f7f4-42cd-db92-e310353598be"
   },
   "outputs": [],
   "source": [
    "# Mejores parámetros para datos NO estandarizados\n",
    "linear_best_params_nostd = best_params_nostd\n",
    "linear_best_params_nostd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuUgQzzYyNW1"
   },
   "source": [
    "Para medir el desempeño del modelo y saber si se está incurriendo en alguna clase de sobre o sub ajuste, a continuación se calculan los siguientes puntajes según las métricas de RMSE, MAE y R2:\n",
    "\n",
    "* Desempeño durante la selección de hiperparámetros:\n",
    "  * Puntaje sobre el conjunto train.\n",
    "  * Puntaje del mejor modelo en un paradigma de Cross Validation.\n",
    "  * Puntaje del mejor modelo probeniente del GridSearch sobre el conjunto de validación.\n",
    "  * Puntaje del mejor modelo probeniente del GridSearch sobre el conjunto de prueba.\n",
    "\n",
    "* Desempeño durante el entrenamiento total:\n",
    " * Puntaje del mejor modelo sobre el conjunto total de datos.\n",
    " * Puntaje del mejor modelo sobre el conjunto total de datos en un paradigma de Cross Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tj2jkMwAGrHb"
   },
   "source": [
    "A continuación, se calculan las métricas de desempeño para el modelo Lasso entrenado con datos estandarizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "JLAJAYEE2q8O",
    "outputId": "f658f917-d624-465c-9efb-9701cde3e4bd"
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------\n",
    "# Validación de desempeño del modelo durante la selección de hiperparámetros para datos estandarizados\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "#----Rmse, Mae y R2----\n",
    "y_data  = [y_train, y_val, y_test]\n",
    "y_preds = [y_train_pred, y_val_pred, y_test_pred]\n",
    "\n",
    "labels_rmse = ['Rmse Train', 'Rmse Validation', 'Rmse Test', 'Rmse CV', 'Rmse full model', 'Rmse CV full model']\n",
    "labels_mae  = ['Mae Train', 'Mae Validation', 'Mae Test', 'Mae CV', 'Mae full model', 'Mae CV full model']\n",
    "labels_r2   = ['R2 Train', 'R2 Validation', 'R2 Test', 'R2 CV', 'R2 full model', 'R2 CV full model']\n",
    "\n",
    "rmse = [mean_squared_error(y_data[i], y_preds[i],squared=False) for i in range(len(y_preds))]\n",
    "mae  = [mean_absolute_error(y_data[i], y_preds[i]) for i in range(len(y_preds))]\n",
    "r2   = [r2_score(y_data[i], y_preds[i]) for i in range(len(y_preds))]\n",
    "\n",
    "#----puntaje por validación cruzada----\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state = 77)\n",
    "cv_scores_hy     = cross_val_score(best_model, X_train_scaled, y_train_scaled, cv=kfold, scoring='neg_root_mean_squared_error')\n",
    "re_scorer_hy     = make_scorer(r2_score)\n",
    "cv_scores_r2_hy  = cross_val_score(best_model, X_train_scaled, y_train_scaled, cv=kfold, scoring=re_scorer_hy)\n",
    "cv_scores_mae_hy = cross_val_score(best_model, X_train_scaled, y_train_scaled, cv=kfold, scoring=make_scorer(mean_absolute_error))\n",
    "\n",
    "cv_scores_hy    = - cv_scores_hy\n",
    "cv_rmse_mean_hy = cv_scores_hy.mean()\n",
    "cv_mae_mean_hy  = cv_scores_mae_hy.mean()\n",
    "cv_r2_mean_hy   = cv_scores_r2_hy.mean()\n",
    "\n",
    "rmse.append(cv_rmse_mean_hy)\n",
    "mae.append(cv_mae_mean_hy)\n",
    "r2.append(cv_r2_mean_hy)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Validación de desempeño del modelo durante entrenamiento sobre todos los datos con estandarización\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "#----Rmse y R2----\n",
    "rmse.append( mean_squared_error(y_data_full, y_pred_full_lin, squared=False) )\n",
    "mae.append( mean_absolute_error(y_data_full, y_pred_full_lin) )\n",
    "r2.append( r2_score(y_data_full, y_pred_full_lin) )\n",
    "\n",
    "#----puntaje por validación cruzada----\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state = 77)\n",
    "cv_scores = cross_val_score(best_bagg_model_full, X_encoded, y_data_full, cv=kfold, scoring='neg_root_mean_squared_error')\n",
    "re_scorer = make_scorer(r2_score)\n",
    "cv_scores_r2 = cross_val_score(best_bagg_model_full, X_encoded, y_data_full, cv=kfold, scoring=re_scorer)\n",
    "cv_scores_mae = cross_val_score(best_bagg_model_full, X_encoded, y_data_full, cv=kfold, scoring=make_scorer(mean_absolute_error))\n",
    "\n",
    "cv_scores    = - cv_scores\n",
    "cv_rmse_mean = cv_scores.mean()\n",
    "cv_mae_mean  = cv_scores_mae.mean()\n",
    "cv_r2_mean   = cv_scores_r2.mean()\n",
    "\n",
    "rmse.append(cv_rmse_mean)\n",
    "mae.append(cv_mae_mean)\n",
    "r2.append(cv_r2_mean)\n",
    "\n",
    "#------------------------------\n",
    "# Puntajes generales del modelo\n",
    "#------------------------------\n",
    "linear_stats = pd.DataFrame({'Rmse type' : labels_rmse,\n",
    "                           'Rmse value': rmse,\n",
    "                           'Mae type'  : labels_mae,\n",
    "                           'Mae value' : mae,\n",
    "                           'R2 type'   : labels_r2,\n",
    "                           'R2 value'  : r2})\n",
    "\n",
    "linear_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRCYduABGyrg"
   },
   "source": [
    "A continuación, se calculan las métricas de desempeño para el modelo Lasso entrenado sin estandarización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "UldVMJJmE3gK",
    "outputId": "220070d3-5364-4c96-a07c-8ed516756e68"
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------------------\n",
    "# Validación de desempeño del modelo durante la selección de hiperparámetros para datos NO estandarizados\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#----Rmse, Mae y R2----\n",
    "y_data  = [y_train, y_val, y_test]\n",
    "y_preds = [y_train_pred_nostd, y_val_pred_nostd, y_test_pred_nostd]\n",
    "\n",
    "labels_rmse = ['Rmse Train', 'Rmse Validation', 'Rmse Test', 'Rmse CV', 'Rmse full model', 'Rmse CV full model']\n",
    "labels_mae  = ['Mae Train', 'Mae Validation', 'Mae Test', 'Mae CV', 'Mae full model', 'Mae CV full model']\n",
    "labels_r2   = ['R2 Train', 'R2 Validation', 'R2 Test', 'R2 CV', 'R2 full model', 'R2 CV full model']\n",
    "\n",
    "rmse = [mean_squared_error(y_data[i], y_preds[i],squared=False) for i in range(len(y_preds))]\n",
    "mae  = [mean_absolute_error(y_data[i], y_preds[i]) for i in range(len(y_preds))]\n",
    "r2   = [r2_score(y_data[i], y_preds[i]) for i in range(len(y_preds))]\n",
    "\n",
    "#----puntaje por validación cruzada----\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state = 77)\n",
    "cv_scores_hy     = cross_val_score(best_model_nostd, X_train, y_train, cv=kfold, scoring='neg_root_mean_squared_error')\n",
    "re_scorer_hy     = make_scorer(r2_score)\n",
    "cv_scores_r2_hy  = cross_val_score(best_model_nostd, X_train, y_train, cv=kfold, scoring=re_scorer_hy)\n",
    "cv_scores_mae_hy = cross_val_score(best_model_nostd, X_train, y_train, cv=kfold, scoring=make_scorer(mean_absolute_error))\n",
    "\n",
    "cv_scores_hy    = - cv_scores_hy\n",
    "cv_rmse_mean_hy = cv_scores_hy.mean()\n",
    "cv_mae_mean_hy  = cv_scores_mae_hy.mean()\n",
    "cv_r2_mean_hy   = cv_scores_r2_hy.mean()\n",
    "\n",
    "rmse.append(cv_rmse_mean_hy)\n",
    "mae.append(cv_mae_mean_hy)\n",
    "r2.append(cv_r2_mean_hy)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Validación de desempeño del modelo durante entrenamiento sobre todos los datos SIN estandarización\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "#----Rmse y R2----\n",
    "rmse.append( mean_squared_error(y_data_full, y_pred_full_nostd, squared=False) )\n",
    "mae.append( mean_absolute_error(y_data_full, y_pred_full_nostd) )\n",
    "r2.append( r2_score(y_data_full, y_pred_full_nostd) )\n",
    "\n",
    "#----puntaje por validación cruzada----\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state = 77)\n",
    "cv_scores = cross_val_score(best_bagg_model_full_nostd, X_encoded, y_data_full, cv=kfold, scoring='neg_root_mean_squared_error')\n",
    "re_scorer = make_scorer(r2_score)\n",
    "cv_scores_r2 = cross_val_score(best_bagg_model_full_nostd, X_encoded, y_data_full, cv=kfold, scoring=re_scorer)\n",
    "cv_scores_mae = cross_val_score(best_bagg_model_full_nostd, X_encoded, y_data_full, cv=kfold, scoring=make_scorer(mean_absolute_error))\n",
    "\n",
    "cv_scores    = - cv_scores\n",
    "cv_rmse_mean = cv_scores.mean()\n",
    "cv_mae_mean  = cv_scores_mae.mean()\n",
    "cv_r2_mean   = cv_scores_r2.mean()\n",
    "\n",
    "rmse.append(cv_rmse_mean)\n",
    "mae.append(cv_mae_mean)\n",
    "r2.append(cv_r2_mean)\n",
    "\n",
    "#------------------------------\n",
    "# Puntajes generales del modelo\n",
    "#------------------------------\n",
    "linear_stats_nostd = pd.DataFrame({'Rmse type' : labels_rmse,\n",
    "                                   'Rmse value': rmse,\n",
    "                                   'Mae type'  : labels_mae,\n",
    "                                   'Mae value' : mae,\n",
    "                                   'R2 type'   : labels_r2,\n",
    "                                   'R2 value'  : r2})\n",
    "\n",
    "linear_stats_nostd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdOpMsOD3Ytj"
   },
   "source": [
    "Para hacernos una idea visual de la calidad de las predicciones de nuestro modelo, graficamos en el eje X los valores reales del conjunto target de prueba y del conjunto de datos en total, mientras que en el eje y, destinamos los valores predichos para la variable target en el conjunto de prueba y en el conjunto de datos total. A continuación, se grafican los resultados obtenidos para el modelo estandarizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "RRAQGuV30cWk",
    "outputId": "b1d8099d-bfe4-4a6e-9d34-a775774d4029"
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------\n",
    "# Visualización comparativa de los datos de la variable target y predicha\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "x_test_plot = y_test.values\n",
    "y_test_plot = y_test_pred.reshape(-1,1)\n",
    "\n",
    "x_full_plot = y_data_full.values\n",
    "y_full_plot = y_pred_full_lin.reshape(-1,1)\n",
    "\n",
    "print('Test size = ', x_test_plot.shape)\n",
    "print('Full data size =', x_full_plot.shape)\n",
    "\n",
    "r2_test = round(r2_score(x_test_plot , y_test_plot),4)\n",
    "rmse_test = np.sqrt(mean_squared_error(x_test_plot , y_test_plot, squared=False))\n",
    "r2_full = round(r2_score(x_full_plot, y_full_plot),4)\n",
    "rmse_full = np.sqrt(mean_squared_error(x_full_plot, y_full_plot, squared=False))\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(10,4))\n",
    "ax[0].scatter(x_test_plot , y_test_plot, c='blue', alpha=0.5, label=f'R2: {r2_test:.2f}\\nRMSE: {rmse_test:.2f}')\n",
    "ax[0].plot(x_test_plot,x_test_plot, color='black')\n",
    "ax[0].set_title('Conjunto target test vs predicciones')\n",
    "ax[0].grid(True)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].scatter(x_full_plot,y_full_plot, c='green', alpha=0.5, label=f'R2: {r2_full:.2f}\\nRMSE: {rmse_full:.2f}')\n",
    "ax[1].plot(x_full_plot,x_full_plot, color='black')\n",
    "ax[1].set_title('Variable target vs predicciones')\n",
    "ax[1].grid(True)\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrRWzK1eQ0VA"
   },
   "source": [
    "# **4. Análisis comparativo de modelos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5_k9E0a8bfl"
   },
   "source": [
    "----\n",
    "Análisis comparativo de modelos\n",
    "\n",
    "---\n",
    "\n",
    "La presente sección está destinada a analizar los resultados obtenidos en el presente proyecto ofreciendo una comparación entre los dos modelos seleccionados para modelar los datos de nuestro problema. Aquí, procederemos a analizar las métricas de desempeño así como la importancia de cada variable dentro del dataset según el modelo Lasso. En general, aquí se pretende responder a las siguientes preguntas:\n",
    "\n",
    "* **Pregunta 1:**¿Cuál es el grado de la transformación polinomial que fue seleccionado utilizando la técnica de validación?\n",
    "\n",
    "* **Pregunta 2:**¿Cuál fue el valor de α que fue seleccionado utilizando la técnica de validación para la regresión Lasso?\n",
    "\n",
    "* **Pregunta 3:**A partir de la tabla comparativa, ¿cuál modelo ofrece el mejor rendimiento sobre el conjunto test? ¿Qué interpretación puedes darles a los valores obtenidos sobre las métricas de rendimiento?\n",
    "\n",
    "* **Pregunta 4:**¿Cuáles variables fueron seleccionadas con el modelo Lasso? A partir de estas, ¿qué interpretación de cara al problema puedes dar? Reflexiona sobre cómo este nuevo conocimiento podría ayudar a tomar decisiones en el contexto del problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXoP2l4R9imk"
   },
   "source": [
    "Para responder la **Pregunta 1**, hemos empleado una técnica de validación cruzada y búsqueda de hiperparámetros por medio de la técnica GridSearch, donde el grado del polinomio que mejor desempeño tuvo fue `degree = 3`. Ya que nuestro modelo polinomial fue planteado como una regresión polinomial, el parámetro de regresión que mejor desempeño tuvo fue `alpha = 0.01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ArObBdnA-DvV",
    "outputId": "79bd8ec3-f0e1-4a70-e5a1-f0bc62919497"
   },
   "outputs": [],
   "source": [
    "poly_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmO745H8-Lg6"
   },
   "source": [
    "En el caso de la regresión Lasso, los lineamientos del presente proyecto dictaban que el parámetro de regularización fuese uno de los siguientes valores `alpha = [1,2,3,4,5]`. No obstante, éste modelo presenta la particularidad de que si una estandarización de los datos de entrada es efectuada, el modelo Lasso no tendrá buen desempeño si el parámetro de regularización es mayor a uno, produciendo resultados con precisión nula. En caso contrario, si ninguna estandarización es efectuada, el modelo Lasso si puede explicar cierto porcentaje de los datos.\n",
    "\n",
    "Sin embargo, bien sea que se emplee un modelo con estandarización o sin estandarización, la regresión Lasso siempre prefiere usar el parámetro `alpha` con el menor valor posible. Por ejemplo, si se emplea una estandarización de los datos, la regularización óptima es para el valor `alpha = 0.001`, mientras que el modelo Lasso sin estandarización se tiene  `alpha = 1`.\n",
    "\n",
    "después de realizar muchos entrenamientos con éstos valores el modelo lineal Lasso no fue capaz de explicar el patrón de los datos en nuestra variable de interés, la frecuencia de alquiler. Los resultados fueron sumamente pobres y la precisión del modelo fué casi nula.\n",
    "\n",
    "En general, los modelos con los modelos Lasso fue posible hacer del modelo lo suficientemente flexible para que explicara aproximadamente el 40% de la variabilidad de los datos. La explicación concerniente a la preferencia del modelo Lasso hacia valores de `alpha` pequeños, se debe a que el modelo lineal al ser un modelo tan simple, al verse regularizado fuertemente inhibe aún más su capacidad para adaptarse a los datos ya que la regularizción es un proceso de \"hacer énfasis\" en el uso de parámetros pequeños para el modelo. Así, si los parámetros son forzados a tener valores pequeños. En este sentido, el modelo es más propenso a errar  y dentro del contexto del modelo lineal, el hecho que los parámetros sean forzados a ser cercanos a cero, el modelo representa un hiperplano que no posee \"inclinación\" alguna, siendo ésta \"inclinación\" la forma en la que el modelo se ajusta a  los datos. Es decir, inhibir la \"inclinación\" del hiperplano es equivalente a no permitir que el modelo se ajuste a los datos.\n",
    "\n",
    "Note además que si `alpha` es mayor a uno, se está amplificando el valor del error que el modelo naturalmente posee, así, un valor de `alpha` mayor a uno generará un modelo con una cantidad de error considerable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "paBztUbDBl5Z",
    "outputId": "13a1cdba-6cd7-418e-c43b-2531b773c0d2"
   },
   "outputs": [],
   "source": [
    "linear_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sUfBsOErJhOO",
    "outputId": "a686dacd-82cf-4c9a-8e25-825702fcfced"
   },
   "outputs": [],
   "source": [
    "linear_best_params_nostd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8gAIS0wJndX"
   },
   "source": [
    "Para responder a la **Pregunta 3**, a continuación se presentan las tablas comparativas para cada modelo usado en el proyecto, donde se han calculado el valor de RMA, MAE y R2.\n",
    "\n",
    "Éstos resultados ilustran que ninguno de los modelos es lo suficientemente adecuado para modelar los datos ya que en general, solo el `40%` de la variabilidad de los datos es explicada por los modelos lineales, y el `48%` en el caso del modelo polinomial. Más aún, según el MAE y el RMSE, para los modelos lineales se tiene que en promedio el error de la predicción se encuentra entre `98 y 137 alquileres`. Es decir, el error  promedio para las predicciones de la cantidad de alquileres de bicicletas por clientes se encuentra ente 100 y 137, un error bastante considerable ya que estos valores se encuentran dentro de la escala  de los datos y teniendo en cuenta que la frecuencia/cantidad de alquileres por cliente promedio es igual a `182`.\n",
    "\n",
    "Aún así, si observamos las métricas de desempeño para los conjuntos de test, para el desempeño del modelo sobre el conjunto completo de datos, y los valores obtenidos para las validaciones cruzadas tanto en la búsqueda de hiperparámetros como en la evaluación cruzada del modelo sobre el conjunto completo de datos, la jerarquía de eficiencia de mayor a menor para nuestros modelos es:\n",
    "\n",
    "1. Modelo polinomial\n",
    "2. Modelo Lasso con estandarización\n",
    "3. Modelo Lasso sin estandarización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "E9-8m3bbmyIh",
    "outputId": "5d9baf2a-31d6-4dbf-b4d6-1d0d0aa4f02f"
   },
   "outputs": [],
   "source": [
    "linear_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "mu3c7KKKJ30_",
    "outputId": "30501c0b-ade7-4bbd-ca47-f20c4aa3f18a"
   },
   "outputs": [],
   "source": [
    "linear_stats_nostd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "X7jwUMtamzL2",
    "outputId": "d795c45e-ef13-42e4-efd2-b983ee560c51"
   },
   "outputs": [],
   "source": [
    "poly_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMpnZ2y4NHrn"
   },
   "source": [
    "Para abordar la **Pregunta 5**, implementamos la técnica de Eliminación Recursiva de Características (RFE, por sus siglas en inglés), que es un método de selección de características. Esta técnica busca identificar el subconjunto óptimo de características, eliminando progresivamente las menos significativas y reajustando el modelo en cada paso, para así establecer una jerarquía de importancia basada en la contribución de cada característica. Posteriormente, examinamos los valores resultantes para los parámetros del modelo Lasso estandarizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "i1NF7WN1k1lI",
    "outputId": "f50d8079-301a-4fb7-d8d8-5248c7ca7e0e"
   },
   "outputs": [],
   "source": [
    "lasso_model = Lasso(alpha=0.1)\n",
    "\n",
    "lasso_model = best_linear_model\n",
    "selector = RFE(estimator=lasso_model, n_features_to_select=3, step=1)\n",
    "selector = selector.fit(X_full_scaled, y_full_scaled)\n",
    "\n",
    "selected_features = selector.support_\n",
    "feature_ranking = selector.ranking_\n",
    "\n",
    "df_ranking = pd.DataFrame({'Característica': X_encoded.columns,\n",
    "                            'Ranking': feature_ranking})\n",
    "df_ranking_sorted = df_ranking.sort_values(by='Ranking')\n",
    "\n",
    "print(\"Tabla de Ranking de Características:\")\n",
    "df_ranking_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "m_hFNyMjOjy2",
    "outputId": "83e3f1f5-c678-40c3-de53-d936f376f650"
   },
   "outputs": [],
   "source": [
    "lasso_model = Lasso(alpha=0.1)\n",
    "\n",
    "lasso_model = best_linear_model_nostd\n",
    "selector = RFE(estimator=lasso_model, n_features_to_select=3, step=1)\n",
    "selector = selector.fit(X_full_scaled, y_full_scaled)\n",
    "\n",
    "selected_features = selector.support_\n",
    "feature_ranking = selector.ranking_\n",
    "\n",
    "df_ranking = pd.DataFrame({'Característica': X_encoded.columns,\n",
    "                            'Ranking': feature_ranking})\n",
    "df_ranking_sorted = df_ranking.sort_values(by='Ranking')\n",
    "\n",
    "print(\"Tabla de Ranking de Características:\")\n",
    "df_ranking_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "iqRkZFCri4Ka",
    "outputId": "54e45d61-6e33-4290-bc6e-4847285400ae"
   },
   "outputs": [],
   "source": [
    "relevancia_caracteristicas = pd.DataFrame({'Variable':X_encoded.columns, 'Relevancia':best_linear_model.coef_})\n",
    "relevancia_caracteristicas['Relevancia'] = relevancia_caracteristicas['Relevancia'].abs()\n",
    "relevancia_caracteristicas.sort_values(by='Relevancia', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "02GuaKLhOS1A",
    "outputId": "f5fcb37b-da42-4ba1-e9a2-22313b8fb687"
   },
   "outputs": [],
   "source": [
    "relevancia_caracteristicas = pd.DataFrame({'Variable':X_encoded.columns, 'Relevancia':best_linear_model_nostd.coef_})\n",
    "relevancia_caracteristicas['Relevancia'] = relevancia_caracteristicas['Relevancia'].abs()\n",
    "relevancia_caracteristicas.sort_values(by='Relevancia', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znhKSbV9OeTY"
   },
   "source": [
    "Al examinar los resultados jerárquicos previos, se constata que aunque los modelos lineales presentan niveles parecidos de precisión o rendimiento, cada uno aporta una visión distinta sobre la interpretación del problema. Por ejemplo, al estandarizar los datos, el modelo Lasso revela que los cuatro factores clave en la determinación del número de alquileres de bicicletas son, en orden, `\"momento del día, sensación térmica, humedad y temperatura\"`. En contraste, el modelo Lasso sin estandarizar indica que las cuatro variables más relevantes son `\"humedad, momento del día, temporada y sensación térmica\"`.\n",
    "\n",
    "Aunque las interpretaciones de ambos modelos son muy parecidas, existen pequeñas diferencias. Sin embargo, al analizar en profundidad, se deduce que la probabilidad de alquiler de bicicletas está influenciada por el flujo de personas en la ciudad, lo cual a su vez depende del momento del día por los horarios laborales y de actividad, y por las condiciones climáticas. Además, no es ilógico considerar que la \"calidad del clima\" sea una variable menos crucial para el modelo, dado que esta depende de factores más fundamentales como la temperatura y la humedad, y por ende, de la temporada.\n",
    "\n",
    "Por lo tanto, si aceptamos que el \"factor humano\" y las \"condiciones meteorológicas\" son los principales determinantes en la cantidad de alquileres, las variables relacionadas a estos factores son `\"humedad, momento del día, temporada y sensación térmica\"`. Así, el modelo Lasso no regularizado se perfila como el que proporciona la interpretación más acertada del problema, ya que este incluye en sus cuatro variables principales a la temperatura, excluyendo una variable tan significativa como la temporada del año.\n",
    "\n",
    "En conclusión, dentro del contexto del problema, el modelo Lasso no estandarizado emerge como el más adecuado para comprender el origen de la variación en el número de alquileres de bicicletas, reconociendo que factores humanos como el momento del día y meteorológicos como la humedad, la temporada y la sensación térmica, son las variables más influyentes en el alquiler de bicicletas."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "pi9zkFfoQeGs",
    "RxDw5HwnfQL-",
    "BC8mp6kne4a9",
    "ZIG2r6Rje0-G",
    "SlUL-DF0ex6D",
    "ubUCQk_6QiVg",
    "4gdkTXLnm8J8",
    "JtKRSO0-nOOm",
    "8hEk6S2IoQJc",
    "OAqqYLW8o83e",
    "AuwMmFXmovoa",
    "fly1NKiNQrXX",
    "SGIIZubdbtOc",
    "q6ofjC6PSjm8",
    "wlvTrvR-PLzQ",
    "qsyG3nYVwRs-",
    "IrRWzK1eQ0VA"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
